<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>'bufferization' Dialect - MLIR</title><meta name=description content="Multi-Level IR Compiler Framework"><meta name=generator content="Hugo 0.80.0"><link href=https://mlir.llvm.org/index.xml rel=alternate type=application/rss+xml><link rel=canonical href=https://mlir.llvm.org/docs/Dialects/BufferizationOps/><link rel=stylesheet href=https://mlir.llvm.org/css/theme.css><script src=https://use.fontawesome.com/releases/v5.0.6/js/all.js></script><link rel=stylesheet href=https://mlir.llvm.org/css/chroma.min.css><script src=https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js></script><script src=https://mlir.llvm.org/js/bundle.js></script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$', '$'] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
    }
  });
</script><link rel=apple-touch-icon sizes=180x180 href="/apple-touch-icon.png?v=1"><link rel=icon type=image/png sizes=32x32 href="/favicon-32x32.png?v=1"><link rel=icon type=image/png sizes=16x16 href="/favicon-16x16.png?v=1"><link rel=manifest href="/site.webmanifest?v=1"><link rel=mask-icon href="/safari-pinned-tab.svg?v=1" color=#3775e0><link rel="shortcut icon" href="/favicon.ico?v=1"><meta name=msapplication-TileColor content="#2d89ef"><meta name=theme-color content="#ffffff"><link rel=icon href=/favicon.svg type=image/svg+xml sizes=any><style>:root{}</style></head><body><div class=container><header><h1><div><img src=https://mlir.llvm.org//mlir-logo.png width=40px align=absmiddle>
MLIR</div></h1><p class=description>Multi-Level IR Compiler Framework</p></header><div class=global-menu><nav><ul><li class=parent><a href>Community<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=https://llvm.discourse.group/c/mlir/31>Forums</a></li><li class=child><a href=https://discord.gg/xS7Z362>Chat</a></li></ul></li><li><a href=/getting_started/Debugging/>Debugging Tips</a></li><li><a href=/getting_started/Faq/>FAQ</a></li><li class=parent><a href=https://github.com/llvm/llvm-project/tree/main/mlir>Source<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=/doxygen/>Doxygen</a></li><li class=child><a href=https://github.com/llvm/llvm-project/tree/main/mlir>GitHub</a></li></ul></li><li><a href="https://bugs.llvm.org/buglist.cgi?bug_status=__open__&list_id=177877&order=changeddate%20DESC%2Cpriority%2Cbug_severity&product=MLIR&query_format=specific">Bugs</a></li><li><a href=https://github.com/llvm/mlir-www/tree/main/website/static/LogoAssets>Logo Assets</a></li><li><a href=https://www.youtube.com/MLIRCompiler>Youtube Channel</a></li></ul></nav></div><div class=content-container><main><h1>'bufferization' Dialect</h1><p>Bufferization in MLIR is the process of converting the <code>tensor</code> type to the
<code>memref</code> type.
The <code>bufferization</code> dialect is intended to collect operations/interfaces
specific to the bufferization passes.</p><p>Overview of the bufferization infrastructure and important conceptual
details related to using the MLIR dialect conversion infrastructure can be
found in
<a href=/docs/Bufferization/>bufferization</a> and
<a href=/docs/BufferDeallocationInternals/>buffer
deallocation</a>.</p><p><nav id=TableOfContents><ul><li><a href=#operation-definition>Operation definition</a><ul><li><a href=#bufferizationalloc_tensor-mlirbufferizationalloctensorop><code>bufferization.alloc_tensor</code> (::mlir::bufferization::AllocTensorOp)</a></li><li><a href=#bufferizationclone-mlirbufferizationcloneop><code>bufferization.clone</code> (::mlir::bufferization::CloneOp)</a></li><li><a href=#bufferizationdealloc_tensor-mlirbufferizationdealloctensorop><code>bufferization.dealloc_tensor</code> (::mlir::bufferization::DeallocTensorOp)</a></li><li><a href=#bufferizationto_memref-mlirbufferizationtomemrefop><code>bufferization.to_memref</code> (::mlir::bufferization::ToMemrefOp)</a></li><li><a href=#bufferizationto_tensor-mlirbufferizationtotensorop><code>bufferization.to_tensor</code> (::mlir::bufferization::ToTensorOp)</a></li></ul></li></ul></nav><h2 id=operation-definition>Operation definition&nbsp;<a class=headline-hash href=#operation-definition>¶</a></h2><h3 id=bufferizationalloc_tensor-mlirbufferizationalloctensorop><code>bufferization.alloc_tensor</code> (::mlir::bufferization::AllocTensorOp)&nbsp;<a class=headline-hash href=#bufferizationalloc_tensor-mlirbufferizationalloctensorop>¶</a></h3><p>buffer allocation in tensor land</p><p><code>bufferization.alloc_tensor</code> materializes an uninitialized tensor with a
given shape (dynamic or static). It always bufferizes to a new buffer
allocation of the given shape. The optional <code>copy</code> operand specifies the
contents of the tensors. If no <code>copy</code> operand is specified, reading from the
result of an <code>alloc_tensor</code> op yields an undefined value.</p><p>If <code>copy</code> is specified, no dynamic sizes should be passed, since they are
the same as the dynamic sizes of the <code>copy</code> operand.</p><p><code>alloc_tensor</code> is a helper op for bufferization. The operation is provided
as an anchor that marks the beginning of a new tensor SSA use-def chain. It
can be used to control in-place bufferization decisions during One-Shot
Bufferize: The bufferized result of a <code>bufferization.alloc_tensor</code> does not
alias with any other buffer, so it can be used to resolve read-after-write
conflicts that would have been introduced by the in-place bufferization of
another op.</p><p>The optional <code>memory_space</code> attribute specifies the memory space when
bufferizing this op. The memory space is inferred from <code>copy</code> if specified.
If neither <code>copy</code> nor <code>memory_space</code> is specified, the default memory space
is used during bufferization.</p><p>The optional <code>size_hint</code> operand specifies the number of non-zero elements
for sparse tensors. The value of <code>size_hint</code> should be not less than 1 and
not larger than the linear size of the corresponding dense tensor type. If
this requirement is not met, the behavior of the operator is undefined.</p><p>Both dense and sparse tensor types are supported. The result of a
<code>bufferization.alloc_tensor</code> is a tensor value that can be used like any
other tensor value. In practice, it is often used as the &ldquo;out&rdquo; operand of
another op. Sparse tensor allocations should always be used in a local
construction operation and never escape the function boundary directly.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%c</span> <span class=p>=</span> bufferization<span class=p>.</span>alloc_tensor<span class=p>(</span><span class=nv>%d1</span><span class=p>,</span> <span class=nv>%d2</span><span class=p>)</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f32</span><span class=p>,</span> <span class=nv>#SparseMatrix</span><span class=p>&gt;</span>
<span class=nv>%0</span> <span class=p>=</span> linalg<span class=p>.</span>matmul
  ins<span class=p>(</span><span class=nv>%a</span><span class=p>,</span> <span class=nv>%b</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f32</span><span class=p>,</span> <span class=nv>#SparseMatrix</span><span class=p>&gt;,</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f32</span><span class=p>,</span> <span class=nv>#SparseMatrix</span><span class=p>&gt;)</span>
  outs<span class=p>(</span><span class=nv>%c</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f32</span><span class=p>,</span> <span class=nv>#SparseMatrix</span><span class=p>&gt;)</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f32</span><span class=p>,</span> <span class=nv>#SparseMatrix</span><span class=p>&gt;</span>
<span class=kt>return</span> <span class=nv>%0</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f32</span><span class=p>,</span> <span class=nv>#SparseMatrix</span><span class=p>&gt;</span>
</code></pre></div><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%c</span> <span class=p>=</span> bufferization<span class=p>.</span>alloc_tensor<span class=p>(</span><span class=nv>%d1</span><span class=p>,</span> <span class=nv>%d2</span><span class=p>)</span> <span class=nl>size_hint =</span> <span class=nv>%noe</span>
  <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f32</span><span class=p>,</span> <span class=nv>#SparseMatrix</span><span class=p>&gt;</span>
</code></pre></div><p>Traits: AttrSizedOperandSegments</p><p>Interfaces: BufferizableOpInterface, ReifyRankedShapedTypeOpInterface</p><h4 id=attributes>Attributes:&nbsp;<a class=headline-hash href=#attributes>¶</a></h4><table><thead><tr><th style=text-align:center>Attribute</th><th style=text-align:center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>memory_space</code></td><td style=text-align:center>::mlir::IntegerAttr</td><td>64-bit unsigned integer attribute</td></tr></tbody></table><h4 id=operands>Operands:&nbsp;<a class=headline-hash href=#operands>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>dynamic_sizes</code></td><td>index</td></tr><tr><td style=text-align:center><code>copy</code></td><td>tensor of any type values</td></tr><tr><td style=text-align:center><code>size_hint</code></td><td>index</td></tr></tbody></table><h4 id=results>Results:&nbsp;<a class=headline-hash href=#results>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>tensor of any type values</td></tr></tbody></table><h3 id=bufferizationclone-mlirbufferizationcloneop><code>bufferization.clone</code> (::mlir::bufferization::CloneOp)&nbsp;<a class=headline-hash href=#bufferizationclone-mlirbufferizationcloneop>¶</a></h3><p>Syntax:</p><pre><code>operation ::= `bufferization.clone` $input attr-dict `:` type($input) `to` type($output)
</code></pre><p>Clones the data in the input view into an implicitly defined output view.</p><p>Usage:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%arg1</span> <span class=p>=</span> bufferization<span class=p>.</span>clone <span class=nv>%arg0</span> <span class=p>:</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>f32</span><span class=p>&gt;</span> to <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>f32</span><span class=p>&gt;</span>
</code></pre></div><p>Valid implementations of this operation may alias the input and output
views or create an actual copy. Mutating the source or result
of the clone operation after the clone operation thus leads to undefined
behavior.</p><p>Interfaces: AllocationOpInterface, CopyOpInterface, MemoryEffectOpInterface</p><h4 id=operands-1>Operands:&nbsp;<a class=headline-hash href=#operands-1>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>input</code></td><td>unranked.memref of any type values or memref of any type values</td></tr></tbody></table><h4 id=results-1>Results:&nbsp;<a class=headline-hash href=#results-1>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>output</code></td><td>unranked.memref of any type values or memref of any type values</td></tr></tbody></table><h3 id=bufferizationdealloc_tensor-mlirbufferizationdealloctensorop><code>bufferization.dealloc_tensor</code> (::mlir::bufferization::DeallocTensorOp)&nbsp;<a class=headline-hash href=#bufferizationdealloc_tensor-mlirbufferizationdealloctensorop>¶</a></h3><p>Releases underlying sparse storage format of given tensor</p><p>Syntax:</p><pre><code>operation ::= `bufferization.dealloc_tensor` $tensor attr-dict `:` type($tensor)
</code></pre><p><code>bufferization.dealloc_tensor</code> is a buffer deallocation in tensor land. This
op can be used for manual buffer deallocation. Some bufferizations (such as
One-Shot Bufferize) take care of buffer deallocation, in which case this op
is usually not needed. Details can be found in the documentation of the
respective bufferization passes.</p><p>In case of a dense tensor, this op lowers to a <code>memref.dealloc</code> op during
bufferization.</p><p>In case of a sparse tensor, this op releases the underlying sparse storage
format for a tensor that materialized earlier through a <code>new</code> operation, a
<code>convert</code> operation with annotated destination tensor type (unless the
convert is folded away), or a <code>bufferization.alloc_tensor</code> operation. The
release operation should only be called once for any materialized tensor.
After this operation, any subsequent <code>memref</code> querying operation on the
tensor returns undefined results.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir>bufferization<span class=p>.</span>dealloc_tensor <span class=nv>%tensor</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>1024x1024x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span>
</code></pre></div><p>Interfaces: BufferizableOpInterface</p><h4 id=operands-2>Operands:&nbsp;<a class=headline-hash href=#operands-2>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>tensor</code></td><td>tensor of any type values</td></tr></tbody></table><h3 id=bufferizationto_memref-mlirbufferizationtomemrefop><code>bufferization.to_memref</code> (::mlir::bufferization::ToMemrefOp)&nbsp;<a class=headline-hash href=#bufferizationto_memref-mlirbufferizationtomemrefop>¶</a></h3><p>tensor to memref cast operation</p><p>Syntax:</p><pre><code>operation ::= `bufferization.to_memref` $tensor attr-dict `:` type($memref)
</code></pre><p>Casts a tensor to a memref.</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=c>// Result type is memref&lt;4x?xf32, #layout, 42&gt;
</span><span class=c></span><span class=nv>%12</span> <span class=p>=</span> bufferization<span class=p>.</span>to_memref <span class=nv>%10</span> <span class=p>:</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>4x?x</span><span class=k>f32</span><span class=p>,</span> <span class=nv>#layout</span><span class=p>,</span> <span class=m>42</span><span class=p>&gt;</span>
</code></pre></div><p>Note, that mutating the result of the <code>to_memref</code> operation leads to
undefined behavior.</p><p>This operation is a specialized variant of the built-in
<code>unrealized_conversion_cast</code> and is intended for use in the context of
gradual bufferization.</p><p>Traits: AlwaysSpeculatableImplTrait, SameOperandsAndResultElementType, SameOperandsAndResultShape</p><p>Interfaces: BufferizableOpInterface, ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)</p><p>Effects: MemoryEffects::Effect{}</p><h4 id=operands-3>Operands:&nbsp;<a class=headline-hash href=#operands-3>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>tensor</code></td><td>tensor of any type values</td></tr></tbody></table><h4 id=results-2>Results:&nbsp;<a class=headline-hash href=#results-2>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>memref</code></td><td>unranked.memref of any type values or memref of any type values</td></tr></tbody></table><h3 id=bufferizationto_tensor-mlirbufferizationtotensorop><code>bufferization.to_tensor</code> (::mlir::bufferization::ToTensorOp)&nbsp;<a class=headline-hash href=#bufferizationto_tensor-mlirbufferizationtotensorop>¶</a></h3><p>memref to tensor operation</p><p>Syntax:</p><pre><code>operation ::= `bufferization.to_tensor` $memref attr-dict `:` type($memref)
</code></pre><p>Create a tensor from a <code>memref</code>, making an independent copy of the element
data. The result value is a tensor whose shape and element type match the
memref operand.</p><p>The opposite of this op is <code>to_memref</code>. Together, these two ops are
useful for source/target materializations when doing type conversions
involving tensors and memrefs.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=c>// Produces a value of tensor&lt;4x?xf32&gt; type.
</span><span class=c></span><span class=nv>%12</span> <span class=p>=</span> bufferization<span class=p>.</span>to_tensor <span class=nv>%10</span> <span class=p>:</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>4x?x</span><span class=k>f32</span><span class=p>,</span> <span class=nv>#layout</span><span class=p>,</span> memspace0<span class=p>&gt;</span>
</code></pre></div><p>If tensor load is used in the bufferization steps, mutating the source
buffer after loading leads to undefined behavior.</p><p>Traits: SameOperandsAndResultElementType, SameOperandsAndResultShape</p><p>Interfaces: BufferizableOpInterface</p><h4 id=operands-4>Operands:&nbsp;<a class=headline-hash href=#operands-4>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>memref</code></td><td>unranked.memref of any type values or memref of any type values</td></tr></tbody></table><h4 id=results-3>Results:&nbsp;<a class=headline-hash href=#results-3>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>tensor of any type values</td></tr></tbody></table><div class=edit-meta><br></div><nav class=pagination><a class="nav nav-prev" href=/docs/Dialects/AsyncDialect/ title="'async' Dialect"><i class="fas fa-arrow-left" aria-hidden=true></i>Prev - 'async' Dialect</a>
<a class="nav nav-next" href=/docs/Dialects/ControlFlowDialect/ title="'cf' Dialect">Next - 'cf' Dialect <i class="fas fa-arrow-right" aria-hidden=true></i></a></nav><footer><p class=powered>Powered by <a href=https://gohugo.io>Hugo</a>. Theme by <a href=https://themes.gohugo.io/hugo-theme-techdoc/>TechDoc</a>. Designed by <a href=https://github.com/thingsym/hugo-theme-techdoc>Thingsym</a>.</p></footer></main><div class=sidebar><nav class=slide-menu><ul><li><a href=https://mlir.llvm.org/>Home</a></li><li><a href=/pubs/>MLIR Related Publications</a></li><li><a href=/talks/>Talks</a></li><li><a href=/users/>Users of MLIR</a></li><li class=has-sub-menu><a href=/getting_started/>Getting Started<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/getting_started/ReportingIssues/>Reporting Issues</a></li><li><a href=/getting_started/Debugging/>Debugging Tips</a></li><li><a href=/getting_started/Faq/>FAQ</a></li><li><a href=/getting_started/Contributing/>How to Contribute</a></li><li><a href=/getting_started/DeveloperGuide/>Developer Guide</a></li><li><a href=/getting_started/openprojects/>Open Projects</a></li><li><a href=/getting_started/Glossary/>Glossary</a></li><li><a href=/getting_started/TestingGuide/>Testing Guide</a></li></ul></li><li class="parent has-sub-menu"><a href=/docs/>Code Documentation<span class="mark opened">-</span></a><ul class=sub-menu><li class=has-sub-menu><a href=/docs/Bindings/>Bindings<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Bindings/Python/>MLIR Python Bindings</a></li></ul></li><li class=has-sub-menu><a href=/docs/Tools/>Tools<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Tools/MLIRLSP/>MLIR : Language Server Protocol</a></li><li><a href=/docs/Tools/mlir-reduce/>MLIR Reduce</a></li></ul></li><li><a href=/docs/NVGPUPasses/></a></li><li><a href=/docs/BufferDeallocationInternals/>Buffer Deallocation - Internals</a></li><li><a href=/docs/Bufferization/>Bufferization</a></li><li><a href=/docs/DataLayout/>Data Layout Modeling</a></li><li><a href=/docs/DebugActions/>Debug Actions</a></li><li><a href=/docs/AttributesAndTypes/>Defining Dialect Attributes and Types</a></li><li><a href=/docs/DefiningDialects/>Defining Dialects</a></li><li><a href=/docs/Diagnostics/>Diagnostic Infrastructure</a></li><li><a href=/docs/DialectConversion/>Dialect Conversion</a></li><li class="parent has-sub-menu"><a href=/docs/Dialects/>Dialects<span class="mark opened">-</span></a><ul class=sub-menu><li><a href=/docs/Dialects/MemRefTransformOps/></a></li><li><a href=/docs/Dialects/OpenACCDialect/>'acc' Dialect</a></li><li><a href=/docs/Dialects/Affine/>'affine' Dialect</a></li><li><a href=/docs/Dialects/AMDGPU/>'amdgpu' Dialect</a></li><li><a href=/docs/Dialects/AMX/>'amx' Dialect</a></li><li><a href=/docs/Dialects/ArithOps/>'arith' Dialect</a></li><li><a href=/docs/Dialects/ArmNeon/>'arm_neon' Dialect</a></li><li><a href=/docs/Dialects/ArmSVE/>'arm_sve' Dialect</a></li><li><a href=/docs/Dialects/AsyncDialect/>'async' Dialect</a></li><li class=active><a href=/docs/Dialects/BufferizationOps/>'bufferization' Dialect</a></li><li><a href=/docs/Dialects/ControlFlowDialect/>'cf' Dialect</a></li><li><a href=/docs/Dialects/ComplexOps/>'complex' Dialect</a></li><li><a href=/docs/Dialects/DLTIDialect/>'dlti' Dialect</a></li><li><a href=/docs/Dialects/EmitC/>'emitc' Dialect</a></li><li><a href=/docs/Dialects/Func/>'func' Dialect</a></li><li><a href=/docs/Dialects/GPU/>'gpu' Dialect</a></li><li><a href=/docs/Dialects/IndexOps/>'index' Dialect</a></li><li class=has-sub-menu><a href=/docs/Dialects/Linalg/>'linalg' Dialect<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Dialects/Linalg/OpDSL/>Linalg OpDSL</a></li></ul></li><li><a href=/docs/Dialects/LLVM/>'llvm' Dialect</a></li><li><a href=/docs/Dialects/MathOps/>'math' Dialect</a></li><li><a href=/docs/Dialects/MemRef/>'memref' Dialect</a></li><li><a href=/docs/Dialects/MLProgramOps/>'ml_program' Dialect</a></li><li><a href=/docs/Dialects/NVGPU/>'nvgpu' Dialect</a></li><li><a href=/docs/Dialects/NVVMDialect/>'nvvm' Dialect</a></li><li><a href=/docs/Dialects/OpenMPDialect/>'omp' Dialect</a></li><li><a href=/docs/Dialects/PDLOps/>'pdl' Dialect</a></li><li><a href=/docs/Dialects/PDLInterpOps/>'pdl_interp' Dialect</a></li><li><a href=/docs/Dialects/QuantDialect/>'quant' Dialect</a></li><li><a href=/docs/Dialects/ROCDLDialect/>'rocdl' Dialect</a></li><li><a href=/docs/Dialects/SCFDialect/>'scf' Dialect</a></li><li><a href=/docs/Dialects/ShapeDialect/>'shape' Dialect</a></li><li><a href=/docs/Dialects/SparseTensorOps/>'sparse_tensor' Dialect</a></li><li><a href=/docs/Dialects/TensorOps/>'tensor' Dialect</a></li><li><a href=/docs/Dialects/Vector/>'vector' Dialect</a></li><li><a href=/docs/Dialects/X86Vector/>'x86vector' Dialect</a></li><li><a href=/docs/Dialects/Builtin/>Builtin Dialect</a></li><li><a href=/docs/Dialects/SPIR-V/>SPIR-V Dialect</a></li><li><a href=/docs/Dialects/TOSA/>Tensor Operator Set Architecture (TOSA) Dialect</a></li><li><a href=/docs/Dialects/Transform/>Transform Dialect</a></li></ul></li><li><a href=/docs/Interfaces/>Interfaces</a></li><li><a href=/docs/TargetLLVMIR/>LLVM IR Target</a></li><li><a href=/docs/BytecodeFormat/>MLIR Bytecode Format</a></li><li><a href=/docs/CAPI/>MLIR C API</a></li><li><a href=/docs/LangRef/>MLIR Language Reference</a></li><li><a href=/docs/Canonicalization/>Operation Canonicalization</a></li><li><a href=/docs/OpDefinitions/>Operation Definition Specification (ODS)</a></li><li><a href=/docs/PassManagement/>Pass Infrastructure</a></li><li><a href=/docs/Passes/>Passes</a></li><li><a href=/docs/PatternRewriter/>Pattern Rewriting : Generic DAG-to-DAG Rewriting</a></li><li><a href=/docs/PDLL/>PDLL - PDL Language</a></li><li><a href=/docs/Quantization/>Quantization</a></li><li class=has-sub-menu><a href=/docs/Rationale/>Rationale<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Rationale/RationaleGenericDAGRewriter/>Generic DAG Rewriter Infrastructure Rationale</a></li><li><a href=/docs/Rationale/RationaleLinalgDialect/>Linalg Dialect Rationale: The Case For Compiler-Friendly Custom Operations</a></li><li><a href=/docs/Rationale/Rationale/>MLIR Rationale</a></li><li><a href=/docs/Rationale/MLIRForGraphAlgorithms/>MLIR: Incremental Application to Graph Algorithms in ML Frameworks</a></li><li><a href=/docs/Rationale/RationaleSimplifiedPolyhedralForm/>MLIR: The case for a simplified polyhedral form</a></li><li><a href=/docs/Rationale/SideEffectsAndSpeculation/>Side Effects & Speculation</a></li><li><a href=/docs/Rationale/UsageOfConst/>Usage of 'const' in MLIR, for core IR types</a></li></ul></li><li><a href=/docs/ShapeInference/>Shape Inference</a></li><li><a href=/docs/SPIRVToLLVMDialectConversion/>SPIR-V Dialect to LLVM Dialect conversion manual</a></li><li><a href=/docs/SymbolsAndSymbolTables/>Symbols and Symbol Tables</a></li><li><a href=/docs/DeclarativeRewrites/>Table-driven Declarative Rewrite Rule (DRR)</a></li><li><a href=/docs/Traits/>Traits</a></li><li class=has-sub-menu><a href=/docs/Tutorials/>Tutorials<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Tutorials/CreatingADialect/>Creating a Dialect</a></li><li><a href=/docs/Tutorials/QuickstartRewrites/>Quickstart tutorial to adding MLIR graph rewrite</a></li><li class=has-sub-menu><a href=/docs/Tutorials/Toy/>Toy Tutorial<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Tutorials/Toy/Ch-1/>Chapter 1: Toy Language and AST</a></li><li><a href=/docs/Tutorials/Toy/Ch-2/>Chapter 2: Emitting Basic MLIR</a></li><li><a href=/docs/Tutorials/Toy/Ch-3/>Chapter 3: High-level Language-Specific Analysis and Transformation</a></li><li><a href=/docs/Tutorials/Toy/Ch-4/>Chapter 4: Enabling Generic Transformation with Interfaces</a></li><li><a href=/docs/Tutorials/Toy/Ch-5/>Chapter 5: Partial Lowering to Lower-Level Dialects for Optimization</a></li><li><a href=/docs/Tutorials/Toy/Ch-6/>Chapter 6: Lowering to LLVM and CodeGeneration</a></li><li><a href=/docs/Tutorials/Toy/Ch-7/>Chapter 7: Adding a Composite Type to Toy</a></li></ul></li><li><a href=/docs/Tutorials/UnderstandingTheIRStructure/>Understanding the IR Structure</a></li><li><a href=/docs/Tutorials/DataFlowAnalysis/>Writing DataFlow Analyses in MLIR</a></li></ul></li></ul></li></ul></nav><div class=sidebar-footer></div></div></div><a href=# id=backtothetop-fixed class=backtothetop data-backtothetop-duration=600 data-backtothetop-easing=easeOutQuart data-backtothetop-fixed-fadein=1000 data-backtothetop-fixed-fadeout=1000 data-backtothetop-fixed-bottom=10 data-backtothetop-fixed-right=20><span class="fa-layers fa-fw"><i class="fas fa-circle"></i><i class="fas fa-arrow-circle-up"></i></span></a></div></body></html>