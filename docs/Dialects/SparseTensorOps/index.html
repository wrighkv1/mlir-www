<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>'sparse_tensor' Dialect - MLIR</title><meta name=description content="Multi-Level IR Compiler Framework"><meta name=generator content="Hugo 0.80.0"><link href=https://mlir.llvm.org/index.xml rel=alternate type=application/rss+xml><link rel=canonical href=https://mlir.llvm.org/docs/Dialects/SparseTensorOps/><link rel=stylesheet href=https://mlir.llvm.org/css/theme.css><script src=https://use.fontawesome.com/releases/v5.0.6/js/all.js></script><link rel=stylesheet href=https://mlir.llvm.org/css/chroma.min.css><script src=https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js></script><script src=https://mlir.llvm.org/js/bundle.js></script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$', '$'] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
    }
  });
</script><link rel=apple-touch-icon sizes=180x180 href="/apple-touch-icon.png?v=1"><link rel=icon type=image/png sizes=32x32 href="/favicon-32x32.png?v=1"><link rel=icon type=image/png sizes=16x16 href="/favicon-16x16.png?v=1"><link rel=manifest href="/site.webmanifest?v=1"><link rel=mask-icon href="/safari-pinned-tab.svg?v=1" color=#3775e0><link rel="shortcut icon" href="/favicon.ico?v=1"><meta name=msapplication-TileColor content="#2d89ef"><meta name=theme-color content="#ffffff"><link rel=icon href=/favicon.svg type=image/svg+xml sizes=any><style>:root{}</style></head><body><div class=container><header><h1><div><img src=https://mlir.llvm.org//mlir-logo.png width=40px align=absmiddle>
MLIR</div></h1><p class=description>Multi-Level IR Compiler Framework</p></header><div class=global-menu><nav><ul><li class=parent><a href>Community<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=https://llvm.discourse.group/c/mlir/31>Forums</a></li><li class=child><a href=https://discord.gg/xS7Z362>Chat</a></li></ul></li><li><a href=/getting_started/Debugging/>Debugging Tips</a></li><li><a href=/getting_started/Faq/>FAQ</a></li><li class=parent><a href=https://github.com/llvm/llvm-project/tree/main/mlir>Source<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=/doxygen/>Doxygen</a></li><li class=child><a href=https://github.com/llvm/llvm-project/tree/main/mlir>GitHub</a></li></ul></li><li><a href="https://bugs.llvm.org/buglist.cgi?bug_status=__open__&list_id=177877&order=changeddate%20DESC%2Cpriority%2Cbug_severity&product=MLIR&query_format=specific">Bugs</a></li><li><a href=https://github.com/llvm/mlir-www/tree/main/website/static/LogoAssets>Logo Assets</a></li><li><a href=https://www.youtube.com/MLIRCompiler>Youtube Channel</a></li></ul></nav></div><div class=content-container><main><h1>'sparse_tensor' Dialect</h1><p>The <code>SparseTensor</code> dialect supports all the attributes, types,
operations, and passes that are required to make sparse tensor
types first class citizens within the MLIR compiler infrastructure.
The dialect forms a bridge between high-level operations on sparse
tensors types and lower-level operations on the actual sparse storage
schemes consisting of pointers, indices, and values. Lower-level
support may consist of fully generated code or may be provided by
means of a small sparse runtime support library.</p><p>The concept of <strong>treating sparsity as a property, not a tedious
implementation detail</strong>, by letting a <strong>sparse compiler</strong> generate
sparse code automatically was pioneered for linear algebra by [Bik96]
in MT1 (see <a href=https://www.aartbik.com/sparse.php>https://www.aartbik.com/sparse.php</a>) and formalized
to tensor algebra by [Kjolstad17,Kjolstad20] in the Sparse Tensor
Algebra Compiler (TACO) project (see <a href=http://tensor-compiler.org>http://tensor-compiler.org</a>).</p><p>The MLIR implementation [Biketal22] closely follows the &ldquo;sparse iteration
theory&rdquo; that forms the foundation of TACO. A rewriting rule is applied to
each tensor expression in the Linalg dialect (MLIR&rsquo;s tensor index notation)
where the sparsity of tensors is indicated using the per-dimension level
types dense/compressed together with a specification of the order on the
dimensions (see [Chou18] for an in-depth discussions and possible
extensions to these level types). Subsequently, a topologically sorted
iteration graph, reflecting the required order on indices with respect
to the dimensions of each tensor, is constructed to ensure that all tensors
are visited in natural index order. Next, iteration lattices are
constructed for the tensor expression for every index in topological
order. Each iteration lattice point consists of a conjunction of tensor
indices together with a tensor (sub)expression that needs to be evaluated
for that conjunction. Within the lattice, iteration points are ordered
according to the way indices are exhausted. As such these iteration
lattices drive actual sparse code generation, which consists of a
relatively straightforward one-to-one mapping from iteration lattices
to combinations of for-loops, while-loops, and if-statements. Sparse
tensor outputs that materialize uninitialized are handled with direct
insertions if all parallel loops are outermost or insertions that
indirectly go through a 1-dimensional access pattern expansion
(a.k.a. workspace) where feasible [Gustavson72,Bik96,Kjolstad19].</p><ul><li>[Bik96] Aart J.C. Bik. Compiler Support for Sparse Matrix Computations.
PhD thesis, Leiden University, May 1996.</li><li>[Biketal22] Aart J.C. Bik, Penporn Koanantakool, Tatiana Shpeisman,
Nicolas Vasilache, Bixia Zheng, and Fredrik Kjolstad. Compiler Support
for Sparse Tensor Computations in MLIR. ACM Transactions on Architecture
and Code Optimization, June, 2022. See: <a href=https://dl.acm.org/doi/10.1145/3544559>https://dl.acm.org/doi/10.1145/3544559</a></li><li>[Chou18] Stephen Chou, Fredrik Berg Kjolstad, and Saman Amarasinghe.
Format Abstraction for Sparse Tensor Algebra Compilers. Proceedings of
the ACM on Programming Languages, October 2018.</li><li>[Chou20] Stephen Chou, Fredrik Berg Kjolstad, and Saman Amarasinghe.
Automatic Generation of Efficient Sparse Tensor Format Conversion Routines.
Proceedings of the 41st ACM SIGPLAN Conference on Programming Language
Design and Implementation, June, 2020.</li><li>[Gustavson72] Fred G. Gustavson. Some basic techniques for solving
sparse systems of linear equations. In Sparse Matrices and Their
Applications, pages 41â€“52. Plenum Press, New York, 1972.</li><li>[Kjolstad17] Fredrik Berg Kjolstad, Shoaib Ashraf Kamil, Stephen Chou, David
Lugato, and Saman Amarasinghe. The Tensor Algebra Compiler. Proceedings of
the ACM on Programming Languages, October 2017.</li><li>[Kjolstad19] Fredrik Berg Kjolstad, Peter Ahrens, Shoaib Ashraf Kamil,
and Saman Amarasinghe. Tensor Algebra Compilation with Workspaces,
Proceedings of the IEEE/ACM International Symposium on Code Generation
and Optimization, 2019.</li><li>[Kjolstad20] Fredrik Berg Kjolstad. Sparse Tensor Algebra Compilation.
PhD thesis, MIT, February, 2020.</li></ul><p><nav id=TableOfContents><ul><li><a href=#operation-definition>Operation definition</a><ul><li><a href=#sparse_tensorbinary-mlirsparse_tensorbinaryop><code>sparse_tensor.binary</code> (::mlir::sparse_tensor::BinaryOp)</a></li><li><a href=#sparse_tensorcompress-mlirsparse_tensorcompressop><code>sparse_tensor.compress</code> (::mlir::sparse_tensor::CompressOp)</a></li><li><a href=#sparse_tensorconcatenate-mlirsparse_tensorconcatenateop><code>sparse_tensor.concatenate</code> (::mlir::sparse_tensor::ConcatenateOp)</a></li><li><a href=#sparse_tensorconvert-mlirsparse_tensorconvertop><code>sparse_tensor.convert</code> (::mlir::sparse_tensor::ConvertOp)</a></li><li><a href=#sparse_tensorexpand-mlirsparse_tensorexpandop><code>sparse_tensor.expand</code> (::mlir::sparse_tensor::ExpandOp)</a></li><li><a href=#sparse_tensorforeach-mlirsparse_tensorforeachop><code>sparse_tensor.foreach</code> (::mlir::sparse_tensor::ForeachOp)</a></li><li><a href=#sparse_tensorinsert-mlirsparse_tensorinsertop><code>sparse_tensor.insert</code> (::mlir::sparse_tensor::InsertOp)</a></li><li><a href=#sparse_tensorload-mlirsparse_tensorloadop><code>sparse_tensor.load</code> (::mlir::sparse_tensor::LoadOp)</a></li><li><a href=#sparse_tensornew-mlirsparse_tensornewop><code>sparse_tensor.new</code> (::mlir::sparse_tensor::NewOp)</a></li><li><a href=#sparse_tensornumber_of_entries-mlirsparse_tensornumberofentriesop><code>sparse_tensor.number_of_entries</code> (::mlir::sparse_tensor::NumberOfEntriesOp)</a></li><li><a href=#sparse_tensorout-mlirsparse_tensoroutop><code>sparse_tensor.out</code> (::mlir::sparse_tensor::OutOp)</a></li><li><a href=#sparse_tensorpush_back-mlirsparse_tensorpushbackop><code>sparse_tensor.push_back</code> (::mlir::sparse_tensor::PushBackOp)</a></li><li><a href=#sparse_tensorreduce-mlirsparse_tensorreduceop><code>sparse_tensor.reduce</code> (::mlir::sparse_tensor::ReduceOp)</a></li><li><a href=#sparse_tensorselect-mlirsparse_tensorselectop><code>sparse_tensor.select</code> (::mlir::sparse_tensor::SelectOp)</a></li><li><a href=#sparse_tensorsort_coo-mlirsparse_tensorsortcooop><code>sparse_tensor.sort_coo</code> (::mlir::sparse_tensor::SortCooOp)</a></li><li><a href=#sparse_tensorsort-mlirsparse_tensorsortop><code>sparse_tensor.sort</code> (::mlir::sparse_tensor::SortOp)</a></li><li><a href=#sparse_tensorindices-mlirsparse_tensortoindicesop><code>sparse_tensor.indices</code> (::mlir::sparse_tensor::ToIndicesOp)</a></li><li><a href=#sparse_tensorpointers-mlirsparse_tensortopointersop><code>sparse_tensor.pointers</code> (::mlir::sparse_tensor::ToPointersOp)</a></li><li><a href=#sparse_tensorvalues-mlirsparse_tensortovaluesop><code>sparse_tensor.values</code> (::mlir::sparse_tensor::ToValuesOp)</a></li><li><a href=#sparse_tensorunary-mlirsparse_tensorunaryop><code>sparse_tensor.unary</code> (::mlir::sparse_tensor::UnaryOp)</a></li><li><a href=#sparse_tensoryield-mlirsparse_tensoryieldop><code>sparse_tensor.yield</code> (::mlir::sparse_tensor::YieldOp)</a></li></ul></li><li><a href=#attribute-definition>Attribute definition</a><ul><li><a href=#sparsetensorencodingattr>SparseTensorEncodingAttr</a></li></ul></li></ul></nav><h2 id=operation-definition>Operation definition&nbsp;<a class=headline-hash href=#operation-definition>Â¶</a></h2><h3 id=sparse_tensorbinary-mlirsparse_tensorbinaryop><code>sparse_tensor.binary</code> (::mlir::sparse_tensor::BinaryOp)&nbsp;<a class=headline-hash href=#sparse_tensorbinary-mlirsparse_tensorbinaryop>Â¶</a></h3><p>Binary set operation utilized within linalg.generic</p><p>Syntax:</p><pre><code>operation ::= `sparse_tensor.binary` $x `,` $y `:` attr-dict type($x) `,` type($y) `to` type($output) `\n`
              `overlap` `=` $overlapRegion `\n`
              `left` `=` (`identity` $left_identity^):($leftRegion)? `\n`
              `right` `=` (`identity` $right_identity^):($rightRegion)?
</code></pre><p>Defines a computation within a <code>linalg.generic</code> operation that takes two
operands and executes one of the regions depending on whether both operands
or either operand is nonzero (i.e. stored explicitly in the sparse storage
format).</p><p>Three regions are defined for the operation and must appear in this order:</p><ul><li>overlap (elements present in both sparse tensors)</li><li>left (elements only present in the left sparse tensor)</li><li>right (element only present in the right sparse tensor)</li></ul><p>Each region contains a single block describing the computation and result.
Every non-empty block must end with a sparse_tensor.yield and the return
type must match the type of <code>output</code>. The primary region&rsquo;s block has two
arguments, while the left and right region&rsquo;s block has only one argument.</p><p>A region may also be declared empty (i.e. <code>left={}</code>), indicating that the
region does not contribute to the output. For example, setting both
<code>left={}</code> and <code>right={}</code> is equivalent to the intersection of the two
inputs as only the overlap region will contribute values to the output.</p><p>As a convenience, there is also a special token <code>identity</code> which can be
used in place of the left or right region. This token indicates that
the return value is the input value (i.e. func(%x) => return %x).
As a practical example, setting <code>left=identity</code> and <code>right=identity</code>
would be equivalent to a union operation where non-overlapping values
in the inputs are copied to the output unchanged.</p><p>Example of isEqual applied to intersecting elements only:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%C</span> <span class=p>=</span> bufferization<span class=p>.</span>alloc_tensor<span class=p>...</span>
<span class=nv>%0</span> <span class=p>=</span> linalg<span class=p>.</span>generic <span class=nv>#trait</span>
  ins<span class=p>(</span><span class=nv>%A</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#SparseVector</span><span class=p>&gt;,</span>
      <span class=nv>%B</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#SparseVector</span><span class=p>&gt;)</span>
  outs<span class=p>(</span><span class=nv>%C</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>i8</span><span class=p>,</span> <span class=nv>#SparseVector</span><span class=p>&gt;)</span> <span class=p>{</span>
  <span class=nl>^bb0</span><span class=p>(</span><span class=nv>%a</span><span class=p>:</span> <span class=k>f64</span><span class=p>,</span> <span class=nv>%b</span><span class=p>:</span> <span class=k>f64</span><span class=p>,</span> <span class=nv>%c</span><span class=p>:</span> <span class=k>i8</span><span class=p>)</span> <span class=p>:</span>
    <span class=nv>%result</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>binary <span class=nv>%a</span><span class=p>,</span> <span class=nv>%b</span> <span class=p>:</span> <span class=k>f64</span><span class=p>,</span> <span class=k>f64</span> to <span class=k>i8</span>
      <span class=nl>overlap=</span><span class=p>{</span>
        <span class=nl>^bb0</span><span class=p>(</span><span class=nv>%arg0</span><span class=p>:</span> <span class=k>f64</span><span class=p>,</span> <span class=nv>%arg1</span><span class=p>:</span> <span class=k>f64</span><span class=p>):</span>
          <span class=nv>%cmp</span> <span class=p>=</span> arith<span class=p>.</span>cmpf <span class=s>&#34;oeq&#34;</span><span class=p>,</span> <span class=nv>%arg0</span><span class=p>,</span> <span class=nv>%arg1</span> <span class=p>:</span> <span class=k>f64</span>
          <span class=nv>%ret_i8</span> <span class=p>=</span> arith<span class=p>.</span>extui <span class=nv>%cmp</span> <span class=p>:</span> <span class=k>i1</span> to <span class=k>i8</span>
          sparse_tensor<span class=p>.</span>yield <span class=nv>%ret_i8</span> <span class=p>:</span> <span class=k>i8</span>
      <span class=p>}</span>
      <span class=nl>left=</span><span class=p>{}</span>
      <span class=nl>right=</span><span class=p>{}</span>
    linalg<span class=p>.</span>yield <span class=nv>%result</span> <span class=p>:</span> <span class=k>i8</span>
<span class=p>}</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>i8</span><span class=p>,</span> <span class=nv>#SparseVector</span><span class=p>&gt;</span>
</code></pre></div><p>Example of A+B in upper triangle, A-B in lower triangle:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%C</span> <span class=p>=</span> bufferization<span class=p>.</span>alloc_tensor<span class=p>...</span>
<span class=nv>%1</span> <span class=p>=</span> linalg<span class=p>.</span>generic <span class=nv>#trait</span>
  ins<span class=p>(</span><span class=nv>%A</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;,</span> <span class=nv>%B</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span>
  outs<span class=p>(</span><span class=nv>%C</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span> <span class=p>{</span>
  <span class=nl>^bb0</span><span class=p>(</span><span class=nv>%a</span><span class=p>:</span> <span class=k>f64</span><span class=p>,</span> <span class=nv>%b</span><span class=p>:</span> <span class=k>f64</span><span class=p>,</span> <span class=nv>%c</span><span class=p>:</span> <span class=k>f64</span><span class=p>)</span> <span class=p>:</span>
    <span class=nv>%row</span> <span class=p>=</span> linalg<span class=p>.</span><span class=k>index</span> <span class=m>0</span> <span class=p>:</span> <span class=k>index</span>
    <span class=nv>%col</span> <span class=p>=</span> linalg<span class=p>.</span><span class=k>index</span> <span class=m>1</span> <span class=p>:</span> <span class=k>index</span>
    <span class=nv>%result</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>binary <span class=nv>%a</span><span class=p>,</span> <span class=nv>%b</span> <span class=p>:</span> <span class=k>f64</span><span class=p>,</span> <span class=k>f64</span> to <span class=k>f64</span>
      <span class=nl>overlap=</span><span class=p>{</span>
        <span class=nl>^bb0</span><span class=p>(</span><span class=nv>%x</span><span class=p>:</span> <span class=k>f64</span><span class=p>,</span> <span class=nv>%y</span><span class=p>:</span> <span class=k>f64</span><span class=p>):</span>
          <span class=nv>%cmp</span> <span class=p>=</span> arith<span class=p>.</span>cmpi <span class=s>&#34;uge&#34;</span><span class=p>,</span> <span class=nv>%col</span><span class=p>,</span> <span class=nv>%row</span> <span class=p>:</span> <span class=k>index</span>
          <span class=nv>%upperTriangleResult</span> <span class=p>=</span> arith<span class=p>.</span>addf <span class=nv>%x</span><span class=p>,</span> <span class=nv>%y</span> <span class=p>:</span> <span class=k>f64</span>
          <span class=nv>%lowerTriangleResult</span> <span class=p>=</span> arith<span class=p>.</span>subf <span class=nv>%x</span><span class=p>,</span> <span class=nv>%y</span> <span class=p>:</span> <span class=k>f64</span>
          <span class=nv>%ret</span> <span class=p>=</span> arith<span class=p>.</span>select <span class=nv>%cmp</span><span class=p>,</span> <span class=nv>%upperTriangleResult</span><span class=p>,</span> <span class=nv>%lowerTriangleResult</span> <span class=p>:</span> <span class=k>f64</span>
          sparse_tensor<span class=p>.</span>yield <span class=nv>%ret</span> <span class=p>:</span> <span class=k>f64</span>
      <span class=p>}</span>
      <span class=nl>left=</span>identity
      <span class=nl>right=</span><span class=p>{</span>
        <span class=nl>^bb0</span><span class=p>(</span><span class=nv>%y</span><span class=p>:</span> <span class=k>f64</span><span class=p>):</span>
          <span class=nv>%cmp</span> <span class=p>=</span> arith<span class=p>.</span>cmpi <span class=s>&#34;uge&#34;</span><span class=p>,</span> <span class=nv>%col</span><span class=p>,</span> <span class=nv>%row</span> <span class=p>:</span> <span class=k>index</span>
          <span class=nv>%lowerTriangleResult</span> <span class=p>=</span> arith<span class=p>.</span>negf <span class=nv>%y</span> <span class=p>:</span> <span class=k>f64</span>
          <span class=nv>%ret</span> <span class=p>=</span> arith<span class=p>.</span>select <span class=nv>%cmp</span><span class=p>,</span> <span class=nv>%y</span><span class=p>,</span> <span class=nv>%lowerTriangleResult</span> <span class=p>:</span> <span class=k>f64</span>
          sparse_tensor<span class=p>.</span>yield <span class=nv>%ret</span> <span class=p>:</span> <span class=k>f64</span>
      <span class=p>}</span>
    linalg<span class=p>.</span>yield <span class=nv>%result</span> <span class=p>:</span> <span class=k>f64</span>
<span class=p>}</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span>
</code></pre></div><p>Example of set difference. Returns a copy of A where its sparse structure
is <em>not</em> overlapped by B. The element type of B can be different than A
because we never use its values, only its sparse structure:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%C</span> <span class=p>=</span> bufferization<span class=p>.</span>alloc_tensor<span class=p>...</span>
<span class=nv>%2</span> <span class=p>=</span> linalg<span class=p>.</span>generic <span class=nv>#trait</span>
  ins<span class=p>(</span><span class=nv>%A</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;,</span> <span class=nv>%B</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>i32</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span>
  outs<span class=p>(</span><span class=nv>%C</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span> <span class=p>{</span>
  <span class=nl>^bb0</span><span class=p>(</span><span class=nv>%a</span><span class=p>:</span> <span class=k>f64</span><span class=p>,</span> <span class=nv>%b</span><span class=p>:</span> <span class=k>i32</span><span class=p>,</span> <span class=nv>%c</span><span class=p>:</span> <span class=k>f64</span><span class=p>)</span> <span class=p>:</span>
    <span class=nv>%result</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>binary <span class=nv>%a</span><span class=p>,</span> <span class=nv>%b</span> <span class=p>:</span> <span class=k>f64</span><span class=p>,</span> <span class=k>i32</span> to <span class=k>f64</span>
      <span class=nl>overlap=</span><span class=p>{}</span>
      <span class=nl>left=</span>identity
      <span class=nl>right=</span><span class=p>{}</span>
    linalg<span class=p>.</span>yield <span class=nv>%result</span> <span class=p>:</span> <span class=k>f64</span>
<span class=p>}</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span>
</code></pre></div><p>Traits: AlwaysSpeculatableImplTrait</p><p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)</p><p>Effects: MemoryEffects::Effect{}</p><h4 id=attributes>Attributes:&nbsp;<a class=headline-hash href=#attributes>Â¶</a></h4><table><thead><tr><th style=text-align:center>Attribute</th><th style=text-align:center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>left_identity</code></td><td style=text-align:center>::mlir::UnitAttr</td><td>unit attribute</td></tr><tr><td style=text-align:center><code>right_identity</code></td><td style=text-align:center>::mlir::UnitAttr</td><td>unit attribute</td></tr></tbody></table><h4 id=operands>Operands:&nbsp;<a class=headline-hash href=#operands>Â¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>x</code></td><td>any type</td></tr><tr><td style=text-align:center><code>y</code></td><td>any type</td></tr></tbody></table><h4 id=results>Results:&nbsp;<a class=headline-hash href=#results>Â¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>output</code></td><td>any type</td></tr></tbody></table><h3 id=sparse_tensorcompress-mlirsparse_tensorcompressop><code>sparse_tensor.compress</code> (::mlir::sparse_tensor::CompressOp)&nbsp;<a class=headline-hash href=#sparse_tensorcompress-mlirsparse_tensorcompressop>Â¶</a></h3><p>Compressed an access pattern for insertion</p><p>Syntax:</p><pre><code>operation ::= `sparse_tensor.compress` $values `,` $filled `,` $added `,` $count `into` $tensor `[` $indices `]` attr-dict `:` type($values) `,` type($filled) `,` type($added) `,` type($tensor)
</code></pre><p>Finishes a single access pattern expansion by moving inserted elements
into the sparse storage scheme of the given tensor with the given
indices. The arity of indices is one less than the rank of the tensor,
with the remainder innermost indices defined through the added array.
The values and filled array are reset in a <em>sparse</em> fashion by only
iterating over set elements through an indirection using the added
array, so that the operations are kept proportional to the number of
nonzeros. See the <code>sparse_tensor.expand</code> operation for more details.</p><p>Note that this operation is &ldquo;impure&rdquo; in the sense that even though
the result is modeled through an SSA value, the insertion is eventually
done &ldquo;in place&rdquo;, and referencing the old SSA value is undefined behavior.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%result</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>compress <span class=nv>%values</span><span class=p>,</span> <span class=nv>%filled</span><span class=p>,</span> <span class=nv>%added</span><span class=p>,</span> <span class=nv>%count</span> into <span class=nv>%tensor</span><span class=p>[</span><span class=nv>%i</span><span class=p>]</span>
  <span class=p>:</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>f64</span><span class=p>&gt;,</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>i1</span><span class=p>&gt;,</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;,</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>4x4x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span>
</code></pre></div><p>Interfaces: InferTypeOpInterface</p><h4 id=operands-1>Operands:&nbsp;<a class=headline-hash href=#operands-1>Â¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>values</code></td><td>strided memref of any type values of rank 1</td></tr><tr><td style=text-align:center><code>filled</code></td><td>1D memref of 1-bit signless integer values</td></tr><tr><td style=text-align:center><code>added</code></td><td>1D memref of index values</td></tr><tr><td style=text-align:center><code>count</code></td><td>index</td></tr><tr><td style=text-align:center><code>tensor</code></td><td>sparse tensor of any type values</td></tr><tr><td style=text-align:center><code>indices</code></td><td>index</td></tr></tbody></table><h4 id=results-1>Results:&nbsp;<a class=headline-hash href=#results-1>Â¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>sparse tensor of any type values</td></tr></tbody></table><h3 id=sparse_tensorconcatenate-mlirsparse_tensorconcatenateop><code>sparse_tensor.concatenate</code> (::mlir::sparse_tensor::ConcatenateOp)&nbsp;<a class=headline-hash href=#sparse_tensorconcatenate-mlirsparse_tensorconcatenateop>Â¶</a></h3><p>Concatenates a list of tensors into a single tensor.</p><p>Syntax:</p><pre><code>operation ::= `sparse_tensor.concatenate` $inputs attr-dict `:` type($inputs) `to` type($result)
</code></pre><p>Concatenates a list input tensors and the output tensor with the same rank.
The concatenation happens on the specified <code>dimension</code> (0&lt;= dimension &lt; rank).
The resulting <code>dimension</code> size is the sum of all the input dimension sizes,
while all the other dimensions should have the same size in the input and
output tensors.</p><p>Only statically-sized input tensors are accepted, while the output tensor
can be dynamically-sized.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%0</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>concatenate <span class=nv>%1</span><span class=p>,</span> <span class=nv>%2</span> <span class=p>{</span> <span class=nl>dimension =</span> <span class=m>0</span> <span class=p>:</span> <span class=k>index</span> <span class=p>}</span>
  <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>64x64x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;,</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>64x64x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span> to <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>128x64x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span>
</code></pre></div><p>Traits: AlwaysSpeculatableImplTrait</p><p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)</p><p>Effects: MemoryEffects::Effect{}</p><h4 id=attributes-1>Attributes:&nbsp;<a class=headline-hash href=#attributes-1>Â¶</a></h4><table><thead><tr><th style=text-align:center>Attribute</th><th style=text-align:center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>dimension</code></td><td style=text-align:center>::mlir::IntegerAttr</td><td>index attribute</td></tr></tbody></table><h4 id=operands-2>Operands:&nbsp;<a class=headline-hash href=#operands-2>Â¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>inputs</code></td><td>ranked tensor of any type values</td></tr></tbody></table><h4 id=results-2>Results:&nbsp;<a class=headline-hash href=#results-2>Â¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>ranked tensor of any type values</td></tr></tbody></table><h3 id=sparse_tensorconvert-mlirsparse_tensorconvertop><code>sparse_tensor.convert</code> (::mlir::sparse_tensor::ConvertOp)&nbsp;<a class=headline-hash href=#sparse_tensorconvert-mlirsparse_tensorconvertop>Â¶</a></h3><p>Converts between different tensor types</p><p>Syntax:</p><pre><code>operation ::= `sparse_tensor.convert` $source attr-dict `:` type($source) `to` type($dest)
</code></pre><p>Converts one sparse or dense tensor type to another tensor type. The rank
of the source and destination types must match exactly, and the dimension
sizes must either match exactly or relax from a static to a dynamic size.
The sparse encoding of the two types can obviously be completely different.
The name <code>convert</code> was preferred over <code>cast</code>, since the operation may incur
a non-trivial cost.</p><p>When converting between two different sparse tensor types, only explicitly
stored values are moved from one underlying sparse storage format to
the other. When converting from an unannotated dense tensor type to a
sparse tensor type, an explicit test for nonzero values is used. When
converting to an unannotated dense tensor type, implicit zeroes in the
sparse storage format are made explicit. Note that the conversions can have
non-trivial costs associated with them, since they may involve elaborate
data structure transformations. Also, conversions from sparse tensor types
into dense tensor types may be infeasible in terms of storage requirements.</p><p>Trivial dense-to-dense convert will be removed by canonicalization while
trivial sparse-to-sparse convert will be removed by the sparse codegen. This
is because we use trivial sparse-to-sparse convert to tell bufferization
that the sparse codegen will expand the tensor buffer into sparse tensor
storage.</p><p>Examples:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%0</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>convert <span class=nv>%a</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>32x32x</span><span class=k>f32</span><span class=p>&gt;</span> to <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>32x32x</span><span class=k>f32</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span>
<span class=nv>%1</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>convert <span class=nv>%a</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>32x32x</span><span class=k>f32</span><span class=p>&gt;</span> to <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f32</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span>
<span class=nv>%2</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>convert <span class=nv>%b</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>8x8x</span><span class=k>i32</span><span class=p>,</span> <span class=nv>#CSC</span><span class=p>&gt;</span> to <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>8x8x</span><span class=k>i32</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span>
<span class=nv>%3</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>convert <span class=nv>%c</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>4x8x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span> to <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>4x?x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSC</span><span class=p>&gt;</span>

<span class=c>// The following conversion is not allowed (since it would require a
</span><span class=c>// runtime assertion that the source&#39;s dimension size is actually 100).
</span><span class=c></span><span class=nv>%4</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>convert <span class=nv>%d</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>f64</span><span class=p>&gt;</span> to <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>100x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#SV</span><span class=p>&gt;</span>
</code></pre></div><p>Traits: AlwaysSpeculatableImplTrait, SameOperandsAndResultElementType</p><p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)</p><p>Effects: MemoryEffects::Effect{}</p><h4 id=operands-3>Operands:&nbsp;<a class=headline-hash href=#operands-3>Â¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>source</code></td><td>tensor of any type values</td></tr></tbody></table><h4 id=results-3>Results:&nbsp;<a class=headline-hash href=#results-3>Â¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>dest</code></td><td>tensor of any type values</td></tr></tbody></table><h3 id=sparse_tensorexpand-mlirsparse_tensorexpandop><code>sparse_tensor.expand</code> (::mlir::sparse_tensor::ExpandOp)&nbsp;<a class=headline-hash href=#sparse_tensorexpand-mlirsparse_tensorexpandop>Â¶</a></h3><p>Expands an access pattern for insertion</p><p>Syntax:</p><pre><code>operation ::= `sparse_tensor.expand` $tensor attr-dict `:` type($tensor) `to` type($values) `,` type($filled) `,` type($added)
</code></pre><p>Performs an access pattern expansion for the innermost dimensions of the
given tensor. This operation is useful to implement kernels in which a
sparse tensor appears as output. This technique is known under several
different names and using several alternative implementations,
for example, phase counter [Gustavson72], expanded or switch array
[Pissanetzky84], in phase scan [Duff90], access pattern expansion [Bik96],
and workspaces [Kjolstad19].</p><p>The values and filled array have sizes that suffice for a <em>dense</em> innermost
dimension (e.g. a full row for matrices). The added array and count are used
to store new indices when a false value is encountered in the filled array.
All arrays should be allocated before the loop (possibly even shared between
loops in a future optimization) so that their <em>dense</em> initialization can be
amortized over many iterations. Setting and resetting the dense arrays in
the loop nest itself is kept <em>sparse</em> by only iterating over set elements
through an indirection using the added array, so that the operations are
kept proportional to the number of nonzeros.</p><p>Note that this operation is &ldquo;impure&rdquo; in the sense that even though the
results are modeled through SSA values, the operation relies on a proper
side-effecting context that sets and resets the expanded arrays.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%values</span><span class=p>,</span> <span class=nv>%filled</span><span class=p>,</span> <span class=nv>%added</span><span class=p>,</span> <span class=nv>%count</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>expand <span class=nv>%tensor</span>
  <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>4x4x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span> to <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>f64</span><span class=p>&gt;,</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>i1</span><span class=p>&gt;,</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;</span>
</code></pre></div><h4 id=operands-4>Operands:&nbsp;<a class=headline-hash href=#operands-4>Â¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>tensor</code></td><td>sparse tensor of any type values</td></tr></tbody></table><h4 id=results-4>Results:&nbsp;<a class=headline-hash href=#results-4>Â¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>values</code></td><td>strided memref of any type values of rank 1</td></tr><tr><td style=text-align:center><code>filled</code></td><td>1D memref of 1-bit signless integer values</td></tr><tr><td style=text-align:center><code>added</code></td><td>1D memref of index values</td></tr><tr><td style=text-align:center><code>count</code></td><td>index</td></tr></tbody></table><h3 id=sparse_tensorforeach-mlirsparse_tensorforeachop><code>sparse_tensor.foreach</code> (::mlir::sparse_tensor::ForeachOp)&nbsp;<a class=headline-hash href=#sparse_tensorforeach-mlirsparse_tensorforeachop>Â¶</a></h3><p>Iterates over elements in a tensor</p><p>Syntax:</p><pre><code>operation ::= `sparse_tensor.foreach` `in` $tensor (`init``(`$initArgs^`)`)? attr-dict    `:` type($tensor) (`,` type($initArgs)^)?  (`-&gt;` type($results)^)?  `do` $region
</code></pre><p>Iterates over stored elements in a tensor (which are typically, but not always,
non-zero for sparse tensors) and executes the block.</p><p>For an input tensor with rank n, the block must take n + 1 (and additional loop
carried variables as described below) arguments. The first n arguments must be
Index type, together indicating the current coordinates of the element being visited.
The last argument must have the same type as the
tensor&rsquo;s element type, representing the actual value loaded from the input
tensor at the given coordinates.</p><p><code>sparse_tensor.foreach</code> can also operate on loop-carried variables and returns
the final values after loop termination. The initial values of the variables are
passed as additional SSA operands to the &ldquo;sparse_tensor.foreach&rdquo; following the n + 1
SSA values mentioned above (n coordinate and 1 value).</p><p>The region must terminate with a &ldquo;sparse_tensor.yield&rdquo; that passes the current
values of all loop-carried variables to the next iteration, or to the
result, if at the last iteration. The number and static types of loop-carried
variables may not change with iterations.</p><p>For example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%c0</span> <span class=p>=</span> arith<span class=p>.</span><span class=kt>constant</span> <span class=m>0</span> <span class=p>:</span> <span class=k>i32</span>
<span class=nv>%ret</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>foreach in <span class=nv>%0</span> init<span class=p>(</span><span class=nv>%c0</span><span class=p>):</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>i32</span><span class=p>,</span> <span class=nv>#DCSR</span><span class=p>&gt;,</span> <span class=k>i32</span> <span class=p>-&gt;</span> <span class=k>i32</span> do <span class=p>{</span>
 <span class=nl>^bb0</span><span class=p>(</span><span class=nv>%arg1</span><span class=p>:</span> <span class=k>index</span><span class=p>,</span> <span class=nv>%arg2</span><span class=p>:</span> <span class=k>index</span><span class=p>,</span> <span class=nv>%arg3</span><span class=p>:</span> <span class=k>i32</span><span class=p>,</span> <span class=nv>%iter</span><span class=p>:</span> <span class=k>i32</span><span class=p>):</span>
   <span class=nv>%sum</span> <span class=p>=</span> arith<span class=p>.</span>add <span class=nv>%iter</span><span class=p>,</span> <span class=nv>%arg3</span>
   sparse_tensor<span class=p>.</span>yield <span class=nv>%sum</span>
<span class=p>}</span>
</code></pre></div><p>It is important to note that foreach generated loop iterates over the stored elements
in the storage order. However, no matter what storage order is used, the indices passed
to the block always obey the original dimension order.</p><p>For example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>#COL_MAJOR</span> <span class=p>=</span> <span class=nv>#sparse_tensor.encoding</span><span class=p>&lt;{</span>
  <span class=nl>dimLevelType =</span> <span class=p>[</span> <span class=s>&#34;compressed&#34;</span><span class=p>,</span> <span class=s>&#34;compressed&#34;</span> <span class=p>],</span>
  <span class=nl>dimOrdering =</span> affine_map<span class=p>&lt;(</span>i<span class=p>,</span>j<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>j<span class=p>,</span>i<span class=p>)&gt;</span>
<span class=p>}&gt;</span>

<span class=c>// foreach on a column-major sparse tensor
</span><span class=c></span>sparse_tensor<span class=p>.</span>foreach in <span class=nv>%0</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x3x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#COL_MAJOR</span><span class=p>&gt;</span> do <span class=p>{</span>
 <span class=nl>^bb0</span><span class=p>(</span><span class=nv>%row</span><span class=p>:</span> <span class=k>index</span><span class=p>,</span> <span class=nv>%col</span><span class=p>:</span> <span class=k>index</span><span class=p>,</span> <span class=nv>%arg3</span><span class=p>:</span> <span class=k>f64</span><span class=p>):</span>
    <span class=c>// [%row, %col] -&gt; [0, 0], [1, 0], [2, 0], [0, 1], [1, 1], [2, 1]
</span><span class=c></span><span class=p>}</span>

<span class=nv>#ROW_MAJOR</span> <span class=p>=</span> <span class=nv>#sparse_tensor.encoding</span><span class=p>&lt;{</span>
  <span class=nl>dimLevelType =</span> <span class=p>[</span> <span class=s>&#34;compressed&#34;</span><span class=p>,</span> <span class=s>&#34;compressed&#34;</span> <span class=p>],</span>
<span class=p>}&gt;</span>

<span class=c>// foreach on a row-major sparse tensor
</span><span class=c></span>sparse_tensor<span class=p>.</span>foreach in <span class=nv>%0</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x3x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#ROW_MAJOR</span><span class=p>&gt;</span> do <span class=p>{</span>
 <span class=nl>^bb0</span><span class=p>(</span><span class=nv>%row</span><span class=p>:</span> <span class=k>index</span><span class=p>,</span> <span class=nv>%col</span><span class=p>:</span> <span class=k>index</span><span class=p>,</span> <span class=nv>%arg3</span><span class=p>:</span> <span class=k>f64</span><span class=p>):</span>
    <span class=c>// [%row, %col] -&gt; [0, 0], [0, 1], [1, 0], [1, 1], [2, 0], [2, 1]
</span><span class=c></span><span class=p>}</span>

</code></pre></div><p>Traits: SingleBlockImplicitTerminator<yieldop></p><h4 id=operands-5>Operands:&nbsp;<a class=headline-hash href=#operands-5>Â¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>tensor</code></td><td>tensor of any type values</td></tr><tr><td style=text-align:center><code>initArgs</code></td><td>any type</td></tr></tbody></table><h4 id=results-5>Results:&nbsp;<a class=headline-hash href=#results-5>Â¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>results</code></td><td>any type</td></tr></tbody></table><h3 id=sparse_tensorinsert-mlirsparse_tensorinsertop><code>sparse_tensor.insert</code> (::mlir::sparse_tensor::InsertOp)&nbsp;<a class=headline-hash href=#sparse_tensorinsert-mlirsparse_tensorinsertop>Â¶</a></h3><p>Inserts a value into given sparse tensor</p><p>Syntax:</p><pre><code>operation ::= `sparse_tensor.insert` $value `into` $tensor `[` $indices `]` attr-dict `:` type($tensor)
</code></pre><p>Inserts the given value at given indices into the underlying
sparse storage format of the given tensor with the given indices.
The arity of indices must match the rank of the tensor. This
operation can only be applied when a tensor materializes unintialized
with a <code>bufferization.alloc_tensor</code> operation and the final tensor
is constructed with a <code>load</code> operation that has the <code>hasInserts</code>
attribute set.</p><p>Properties in the sparse tensor type fully describe what kind
of insertion order is allowed. When all dimensions have &ldquo;unique&rdquo;
and &ldquo;ordered&rdquo; properties, for example, insertions should occur in
strict lexicographical index order. Other properties define
different insertion regimens. Inserting in a way contrary to
these properties results in undefined behavior.</p><p>Note that this operation is &ldquo;impure&rdquo; in the sense that even though
the result is modeled through an SSA value, the insertion is eventually
done &ldquo;in place&rdquo;, and referencing the old SSA value is undefined behavior.
This operation is scheduled to be unified with the dense counterpart
<code>tensor.insert</code> that has pure SSA semantics.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%result</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>insert <span class=nv>%val</span> into <span class=nv>%tensor</span><span class=p>[</span><span class=nv>%i</span><span class=p>,</span><span class=nv>%j</span><span class=p>]</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>1024x1024x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span>
</code></pre></div><p>Interfaces: InferTypeOpInterface</p><h4 id=operands-6>Operands:&nbsp;<a class=headline-hash href=#operands-6>Â¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>value</code></td><td>any type</td></tr><tr><td style=text-align:center><code>tensor</code></td><td>sparse tensor of any type values</td></tr><tr><td style=text-align:center><code>indices</code></td><td>index</td></tr></tbody></table><h4 id=results-6>Results:&nbsp;<a class=headline-hash href=#results-6>Â¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>sparse tensor of any type values</td></tr></tbody></table><h3 id=sparse_tensorload-mlirsparse_tensorloadop><code>sparse_tensor.load</code> (::mlir::sparse_tensor::LoadOp)&nbsp;<a class=headline-hash href=#sparse_tensorload-mlirsparse_tensorloadop>Â¶</a></h3><p>Rematerializes tensor from underlying sparse storage format</p><p>Syntax:</p><pre><code>operation ::= `sparse_tensor.load` $tensor (`hasInserts` $hasInserts^)? attr-dict `:` type($tensor)
</code></pre><p>Rematerializes a tensor from the underlying sparse storage format of the
given tensor. This is similar to the <code>bufferization.to_tensor</code> operation
in the sense that it provides a bridge between a bufferized world view
and a tensor world view. Unlike the <code>bufferization.to_tensor</code> operation,
however, this sparse operation is used only temporarily to maintain a
correctly typed intermediate representation during progressive
bufferization.</p><p>The <code>hasInserts</code> attribute denote whether insertions to the underlying
sparse storage format may have occurred, in which case the underlying
sparse storage format needs to be finalized. Otherwise, the operation
simply folds away.</p><p>Note that this operation is &ldquo;impure&rdquo; in the sense that even though
the result is modeled through an SSA value, the operation relies on
a proper context of materializing and inserting the tensor value.</p><p>Examples:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%result</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>load <span class=nv>%tensor</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>8x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#SV</span><span class=p>&gt;</span>

<span class=nv>%1</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>load <span class=nv>%0</span> hasInserts <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>16x32x</span><span class=k>f32</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span>
</code></pre></div><p>Traits: SameOperandsAndResultType</p><p>Interfaces: InferTypeOpInterface</p><h4 id=attributes-2>Attributes:&nbsp;<a class=headline-hash href=#attributes-2>Â¶</a></h4><table><thead><tr><th style=text-align:center>Attribute</th><th style=text-align:center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>hasInserts</code></td><td style=text-align:center>::mlir::UnitAttr</td><td>unit attribute</td></tr></tbody></table><h4 id=operands-7>Operands:&nbsp;<a class=headline-hash href=#operands-7>Â¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>tensor</code></td><td>sparse tensor of any type values</td></tr></tbody></table><h4 id=results-7>Results:&nbsp;<a class=headline-hash href=#results-7>Â¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>tensor of any type values</td></tr></tbody></table><h3 id=sparse_tensornew-mlirsparse_tensornewop><code>sparse_tensor.new</code> (::mlir::sparse_tensor::NewOp)&nbsp;<a class=headline-hash href=#sparse_tensornew-mlirsparse_tensornewop>Â¶</a></h3><p>Materializes a new sparse tensor from given source</p><p>Syntax:</p><pre><code>operation ::= `sparse_tensor.new` $source attr-dict `:` type($source) `to` type($result)
</code></pre><p>Materializes a sparse tensor with contents taken from an opaque pointer
provided by <code>source</code>. For targets that have access to a file system,
for example, this pointer may be a filename (or file) of a sparse
tensor in a particular external storage format. The form of the operation
is kept deliberately very general to allow for alternative implementations
in the future, such as pointers to buffers or runnable initialization
code. The operation is provided as an anchor that materializes a properly
typed sparse tensor with inital contents into a computation.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir>sparse_tensor<span class=p>.</span>new <span class=nv>%source</span> <span class=p>:</span> <span class=p>!</span>Source to <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>1024x1024x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span>
</code></pre></div><p>Traits: AlwaysSpeculatableImplTrait</p><p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)</p><p>Effects: MemoryEffects::Effect{}</p><h4 id=operands-8>Operands:&nbsp;<a class=headline-hash href=#operands-8>Â¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>source</code></td><td>any type</td></tr></tbody></table><h4 id=results-8>Results:&nbsp;<a class=headline-hash href=#results-8>Â¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>sparse tensor of any type values</td></tr></tbody></table><h3 id=sparse_tensornumber_of_entries-mlirsparse_tensornumberofentriesop><code>sparse_tensor.number_of_entries</code> (::mlir::sparse_tensor::NumberOfEntriesOp)&nbsp;<a class=headline-hash href=#sparse_tensornumber_of_entries-mlirsparse_tensornumberofentriesop>Â¶</a></h3><p>Returns the number of entries that are stored in the tensor.</p><p>Syntax:</p><pre><code>operation ::= `sparse_tensor.number_of_entries` $tensor attr-dict `:` type($tensor)
</code></pre><p>Returns the number of entries that are stored in the given sparse tensor.
Note that this is typically the number of nonzero elements in the tensor,
but since explicit zeros may appear in the storage formats, the more
accurate nomenclature is used.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%noe</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>number_of_entries <span class=nv>%tensor</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>64x64x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span>
</code></pre></div><p>Traits: AlwaysSpeculatableImplTrait</p><p>Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface)</p><p>Effects: MemoryEffects::Effect{}</p><h4 id=operands-9>Operands:&nbsp;<a class=headline-hash href=#operands-9>Â¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>tensor</code></td><td>sparse tensor of any type values</td></tr></tbody></table><h4 id=results-9>Results:&nbsp;<a class=headline-hash href=#results-9>Â¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>index</td></tr></tbody></table><h3 id=sparse_tensorout-mlirsparse_tensoroutop><code>sparse_tensor.out</code> (::mlir::sparse_tensor::OutOp)&nbsp;<a class=headline-hash href=#sparse_tensorout-mlirsparse_tensoroutop>Â¶</a></h3><p>Outputs a sparse tensor to the given destination</p><p>Syntax:</p><pre><code>operation ::= `sparse_tensor.out` $tensor `,` $dest attr-dict `:` type($tensor) `,` type($dest)
</code></pre><p>Outputs the contents of a sparse tensor to the destination defined by an
opaque pointer provided by <code>dest</code>. For targets that have access to a file
system, for example, this pointer may specify a filename (or file) for output.
The form of the operation is kept deliberately very general to allow for
alternative implementations in the future, such as sending the contents to
a buffer defined by a pointer.</p><p>Note that this operation is &ldquo;impure&rdquo; in the sense that its behavior
is solely defined by side-effects and not SSA values.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir>sparse_tensor<span class=p>.</span>out <span class=nv>%t</span><span class=p>,</span> <span class=nv>%dest</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>1024x1024x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;,</span> <span class=p>!</span>Dest
</code></pre></div><h4 id=operands-10>Operands:&nbsp;<a class=headline-hash href=#operands-10>Â¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>tensor</code></td><td>sparse tensor of any type values</td></tr><tr><td style=text-align:center><code>dest</code></td><td>any type</td></tr></tbody></table><h3 id=sparse_tensorpush_back-mlirsparse_tensorpushbackop><code>sparse_tensor.push_back</code> (::mlir::sparse_tensor::PushBackOp)&nbsp;<a class=headline-hash href=#sparse_tensorpush_back-mlirsparse_tensorpushbackop>Â¶</a></h3><p>Pushes a value to the back of a given buffer</p><p>Syntax:</p><pre><code>operation ::= `sparse_tensor.push_back` (`inbounds` $inbounds^)? $bufferSizes `,` $inBuffer `,` $value (`,` $n^ )?  attr-dict `:` type($bufferSizes) `,` type($inBuffer) `,` type($value)  (`,` type($n)^ )?
</code></pre><p>Push <code>value</code> to the end of the given sparse tensor storage buffer
<code>inBuffer</code> and update the size of the buffer in <code>bufferSizes[idx]</code>. The
capacity of the buffer is recorded in the memref type of <code>inBuffer </code>. If the
current buffer is full, then <code>inBuffer.realloc</code> is called before pushing the
data to the buffer. This is similar to std::vector push_back.</p><p>The optional input <code>n</code> specifies the number of times to repeately push
the value to the back of the tensor. When <code>n</code> is a compile-time constant,
its value can&rsquo;t be less than 1. If <code>n</code> is a runtime value that is less
than 1, the behavior is undefined. Although using input <code>n</code> is semantically
equivalent to calling push_back n times, it gives compiler more chances to
to optimize the memory reallocation and the filling of the memory with the
same value.</p><p>The <code>inbounds</code> attribute tells the compiler that the insertion won&rsquo;t go
beyond the current storage buffer. This allows the compiler to not generate
the code for capacity check and reallocation. The typical usage will be for
&ldquo;dynamic&rdquo; sparse tensors for which a capacity can be set beforehand.</p><p>Note that this operation is &ldquo;impure&rdquo; in the sense that even though
the result is modeled through an SSA value, referencing the memref
through the old SSA value after this operation is undefined behavior.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%r</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>push_back <span class=nv>%bufferSizes</span><span class=p>,</span> <span class=nv>%buffer</span><span class=p>,</span> <span class=nv>%val</span>
  <span class=p>{</span><span class=nl>idx =</span> <span class=m>0</span> <span class=p>:</span> <span class=k>index</span><span class=p>}</span> <span class=p>:</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;,</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>f64</span><span class=p>&gt;,</span> <span class=k>f64</span>
</code></pre></div><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%r</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>push_back inbounds <span class=nv>%bufferSizes</span><span class=p>,</span> <span class=nv>%buffer</span><span class=p>,</span> <span class=nv>%val</span>
  <span class=p>{</span><span class=nl>idx =</span> <span class=m>0</span> <span class=p>:</span> <span class=k>index</span><span class=p>}</span> <span class=p>:</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;,</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>f64</span><span class=p>&gt;,</span> <span class=k>f64</span>
</code></pre></div><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%r</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>push_back inbounds <span class=nv>%bufferSizes</span><span class=p>,</span> <span class=nv>%buffer</span><span class=p>,</span> <span class=nv>%val</span><span class=p>,</span> <span class=nv>%n</span>
  <span class=p>{</span><span class=nl>idx =</span> <span class=m>0</span> <span class=p>:</span> <span class=k>index</span><span class=p>}</span> <span class=p>:</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;,</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>f64</span><span class=p>&gt;,</span> <span class=k>f64</span>
</code></pre></div><p>Interfaces: InferTypeOpInterface</p><h4 id=attributes-3>Attributes:&nbsp;<a class=headline-hash href=#attributes-3>Â¶</a></h4><table><thead><tr><th style=text-align:center>Attribute</th><th style=text-align:center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>idx</code></td><td style=text-align:center>::mlir::IntegerAttr</td><td>index attribute</td></tr><tr><td style=text-align:center><code>inbounds</code></td><td style=text-align:center>::mlir::UnitAttr</td><td>unit attribute</td></tr></tbody></table><h4 id=operands-11>Operands:&nbsp;<a class=headline-hash href=#operands-11>Â¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>bufferSizes</code></td><td>1D memref of index values</td></tr><tr><td style=text-align:center><code>inBuffer</code></td><td>1D memref of any type values</td></tr><tr><td style=text-align:center><code>value</code></td><td>any type</td></tr><tr><td style=text-align:center><code>n</code></td><td>index</td></tr></tbody></table><h4 id=results-10>Results:&nbsp;<a class=headline-hash href=#results-10>Â¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>outBuffer</code></td><td>1D memref of any type values</td></tr></tbody></table><h3 id=sparse_tensorreduce-mlirsparse_tensorreduceop><code>sparse_tensor.reduce</code> (::mlir::sparse_tensor::ReduceOp)&nbsp;<a class=headline-hash href=#sparse_tensorreduce-mlirsparse_tensorreduceop>Â¶</a></h3><p>Custom reduction operation utilized within linalg.generic</p><p>Syntax:</p><pre><code>operation ::= `sparse_tensor.reduce` $x `,` $y `,` $identity attr-dict `:` type($output) $region
</code></pre><p>Defines a computation with a <code>linalg.generic</code> operation that takes two
operands and an identity value and reduces all values down to a single
result based on the computation in the region.</p><p>The region must contain exactly one block taking two arguments. The block
must end with a sparse_tensor.yield and the output must match the input
argument types.</p><p>Note that this operation is only required for custom reductions beyond the
standard operations (add, mul, and, or, etc). The <code>linalg.generic</code>
<code>iterator_types</code> defines which indices are being reduced. When the associated
operands are used in an operation, a reduction will occur. The use of this
explicit <code>reduce</code> operation is not required in most cases.</p><p>Example of Matrix->Vector reduction using max(product(x_i), 100):</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%cf1</span> <span class=p>=</span> arith<span class=p>.</span><span class=kt>constant</span> <span class=m>1.0</span> <span class=p>:</span> <span class=k>f64</span>
<span class=nv>%cf100</span> <span class=p>=</span> arith<span class=p>.</span><span class=kt>constant</span> <span class=m>100.0</span> <span class=p>:</span> <span class=k>f64</span>
<span class=nv>%C</span> <span class=p>=</span> bufferization<span class=p>.</span>alloc_tensor<span class=p>...</span>
<span class=nv>%0</span> <span class=p>=</span> linalg<span class=p>.</span>generic <span class=nv>#trait</span>
   ins<span class=p>(</span><span class=nv>%A</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#SparseMatrix</span><span class=p>&gt;)</span>
  outs<span class=p>(</span><span class=nv>%C</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#SparseVector</span><span class=p>&gt;)</span> <span class=p>{</span>
  <span class=nl>^bb0</span><span class=p>(</span><span class=nv>%a</span><span class=p>:</span> <span class=k>f64</span><span class=p>,</span> <span class=nv>%c</span><span class=p>:</span> <span class=k>f64</span><span class=p>)</span> <span class=p>:</span>
    <span class=nv>%result</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>reduce <span class=nv>%c</span><span class=p>,</span> <span class=nv>%a</span><span class=p>,</span> <span class=nv>%cf1</span> <span class=p>:</span> <span class=k>f64</span> <span class=p>{</span>
        <span class=nl>^bb0</span><span class=p>(</span><span class=nv>%arg0</span><span class=p>:</span> <span class=k>f64</span><span class=p>,</span> <span class=nv>%arg1</span><span class=p>:</span> <span class=k>f64</span><span class=p>):</span>
          <span class=nv>%0</span> <span class=p>=</span> arith<span class=p>.</span>mulf <span class=nv>%arg0</span><span class=p>,</span> <span class=nv>%arg1</span> <span class=p>:</span> <span class=k>f64</span>
          <span class=nv>%cmp</span> <span class=p>=</span> arith<span class=p>.</span>cmpf <span class=s>&#34;ogt&#34;</span><span class=p>,</span> <span class=nv>%0</span><span class=p>,</span> <span class=nv>%cf100</span> <span class=p>:</span> <span class=k>f64</span>
          <span class=nv>%ret</span> <span class=p>=</span> arith<span class=p>.</span>select <span class=nv>%cmp</span><span class=p>,</span> <span class=nv>%cf100</span><span class=p>,</span> <span class=nv>%0</span> <span class=p>:</span> <span class=k>f64</span>
          sparse_tensor<span class=p>.</span>yield <span class=nv>%ret</span> <span class=p>:</span> <span class=k>f64</span>
      <span class=p>}</span>
    linalg<span class=p>.</span>yield <span class=nv>%result</span> <span class=p>:</span> <span class=k>f64</span>
<span class=p>}</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#SparseVector</span><span class=p>&gt;</span>
</code></pre></div><p>Traits: AlwaysSpeculatableImplTrait, SameOperandsAndResultType</p><p>Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface)</p><p>Effects: MemoryEffects::Effect{}</p><h4 id=operands-12>Operands:&nbsp;<a class=headline-hash href=#operands-12>Â¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>x</code></td><td>any type</td></tr><tr><td style=text-align:center><code>y</code></td><td>any type</td></tr><tr><td style=text-align:center><code>identity</code></td><td>any type</td></tr></tbody></table><h4 id=results-11>Results:&nbsp;<a class=headline-hash href=#results-11>Â¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>output</code></td><td>any type</td></tr></tbody></table><h3 id=sparse_tensorselect-mlirsparse_tensorselectop><code>sparse_tensor.select</code> (::mlir::sparse_tensor::SelectOp)&nbsp;<a class=headline-hash href=#sparse_tensorselect-mlirsparse_tensorselectop>Â¶</a></h3><p>Select operation utilized within linalg.generic</p><p>Syntax:</p><pre><code>operation ::= `sparse_tensor.select` $x attr-dict `:` type($x) $region
</code></pre><p>Defines an evaluation within a <code>linalg.generic</code> operation that takes a single
operand and decides whether or not to keep that operand in the output.</p><p>A single region must contain exactly one block taking one argument. The block
must end with a sparse_tensor.yield and the output type must be boolean.</p><p>Value threshold is an obvious usage of the select operation. However, by using
<code>linalg.index</code>, other useful selection can be achieved, such as selecting the
upper triangle of a matrix.</p><p>Example of selecting A >= 4.0:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%C</span> <span class=p>=</span> bufferization<span class=p>.</span>alloc_tensor<span class=p>...</span>
<span class=nv>%0</span> <span class=p>=</span> linalg<span class=p>.</span>generic <span class=nv>#trait</span>
   ins<span class=p>(</span><span class=nv>%A</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#SparseVector</span><span class=p>&gt;)</span>
  outs<span class=p>(</span><span class=nv>%C</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#SparseVector</span><span class=p>&gt;)</span> <span class=p>{</span>
  <span class=nl>^bb0</span><span class=p>(</span><span class=nv>%a</span><span class=p>:</span> <span class=k>f64</span><span class=p>,</span> <span class=nv>%c</span><span class=p>:</span> <span class=k>f64</span><span class=p>)</span> <span class=p>:</span>
    <span class=nv>%result</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>select <span class=nv>%a</span> <span class=p>:</span> <span class=k>f64</span> <span class=p>{</span>
        <span class=nl>^bb0</span><span class=p>(</span><span class=nv>%arg0</span><span class=p>:</span> <span class=k>f64</span><span class=p>):</span>
          <span class=nv>%cf4</span> <span class=p>=</span> arith<span class=p>.</span><span class=kt>constant</span> <span class=m>4.0</span> <span class=p>:</span> <span class=k>f64</span>
          <span class=nv>%keep</span> <span class=p>=</span> arith<span class=p>.</span>cmpf <span class=s>&#34;uge&#34;</span><span class=p>,</span> <span class=nv>%arg0</span><span class=p>,</span> <span class=nv>%cf4</span> <span class=p>:</span> <span class=k>f64</span>
          sparse_tensor<span class=p>.</span>yield <span class=nv>%keep</span> <span class=p>:</span> <span class=k>i1</span>
      <span class=p>}</span>
    linalg<span class=p>.</span>yield <span class=nv>%result</span> <span class=p>:</span> <span class=k>f64</span>
<span class=p>}</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#SparseVector</span><span class=p>&gt;</span>
</code></pre></div><p>Example of selecting lower triangle of a matrix:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%C</span> <span class=p>=</span> bufferization<span class=p>.</span>alloc_tensor<span class=p>...</span>
<span class=nv>%0</span> <span class=p>=</span> linalg<span class=p>.</span>generic <span class=nv>#trait</span>
   ins<span class=p>(</span><span class=nv>%A</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;)</span>
  outs<span class=p>(</span><span class=nv>%C</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;)</span> <span class=p>{</span>
  <span class=nl>^bb0</span><span class=p>(</span><span class=nv>%a</span><span class=p>:</span> <span class=k>f64</span><span class=p>,</span> <span class=nv>%c</span><span class=p>:</span> <span class=k>f64</span><span class=p>)</span> <span class=p>:</span>
    <span class=nv>%row</span> <span class=p>=</span> linalg<span class=p>.</span><span class=k>index</span> <span class=m>0</span> <span class=p>:</span> <span class=k>index</span>
    <span class=nv>%col</span> <span class=p>=</span> linalg<span class=p>.</span><span class=k>index</span> <span class=m>1</span> <span class=p>:</span> <span class=k>index</span>
    <span class=nv>%result</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>select <span class=nv>%a</span> <span class=p>:</span> <span class=k>f64</span> <span class=p>{</span>
        <span class=nl>^bb0</span><span class=p>(</span><span class=nv>%arg0</span><span class=p>:</span> <span class=k>f64</span><span class=p>):</span>
          <span class=nv>%keep</span> <span class=p>=</span> arith<span class=p>.</span>cmpf <span class=s>&#34;olt&#34;</span><span class=p>,</span> <span class=nv>%col</span><span class=p>,</span> <span class=nv>%row</span> <span class=p>:</span> <span class=k>f64</span>
          sparse_tensor<span class=p>.</span>yield <span class=nv>%keep</span> <span class=p>:</span> <span class=k>i1</span>
      <span class=p>}</span>
    linalg<span class=p>.</span>yield <span class=nv>%result</span> <span class=p>:</span> <span class=k>f64</span>
<span class=p>}</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span>
</code></pre></div><p>Traits: AlwaysSpeculatableImplTrait, SameOperandsAndResultType</p><p>Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface)</p><p>Effects: MemoryEffects::Effect{}</p><h4 id=operands-13>Operands:&nbsp;<a class=headline-hash href=#operands-13>Â¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>x</code></td><td>any type</td></tr></tbody></table><h4 id=results-12>Results:&nbsp;<a class=headline-hash href=#results-12>Â¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>output</code></td><td>any type</td></tr></tbody></table><h3 id=sparse_tensorsort_coo-mlirsparse_tensorsortcooop><code>sparse_tensor.sort_coo</code> (::mlir::sparse_tensor::SortCooOp)&nbsp;<a class=headline-hash href=#sparse_tensorsort_coo-mlirsparse_tensorsortcooop>Â¶</a></h3><p>Sorts the arrays in xs and ys lexicographically on the integral values found in the xs list</p><p>Syntax:</p><pre><code>operation ::= `sparse_tensor.sort_coo` (`stable` $stable^)? $n`,`$xy (`jointly` $ys^)? attr-dict`:` type($xy) (`jointly` type($ys)^)?
</code></pre><p>Sparse_tensor.sort_coo is similar to sparse_tensor.sort, except that all the
<code>xs</code> values and some <code>ys</code> values are put in the linear buffer <code>xy</code>. The
optional index attribute <code>nx</code> provides the number of <code>xs</code> values in <code>xy</code>.
When <code>nx</code> is not explicitly specified, its value is 1. The optional index
attribute <code>ny</code> provides the number of <code>ys</code> values in <code>xy</code>. When <code>ny</code> is not
explicitly specified, its value is 0. This instruction supports a more
efficient way to store the COO definition in sparse tensor type.</p><p>The buffer xy should have a dimension not less than n * (nx + ny) while the
buffers in <code>ys</code> should have a dimension not less than <code>n</code>. The behavior of
the operator is undefined if this condition is not met.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir>sparse_tensor<span class=p>.</span>sort_coo <span class=nv>%n</span><span class=p>,</span> <span class=nv>%x</span> <span class=p>{</span> <span class=nl>nx =</span> <span class=m>2</span> <span class=p>:</span> <span class=k>index</span><span class=p>}</span>
  <span class=p>:</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;</span>
</code></pre></div><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir>sparse_tensor<span class=p>.</span>sort <span class=nv>%n</span><span class=p>,</span> <span class=nv>%xy</span> jointly <span class=nv>%y1</span> <span class=p>{</span> <span class=nl>nx =</span> <span class=m>2</span> <span class=p>:</span> <span class=k>index</span><span class=p>,</span> <span class=nl>ny =</span> <span class=m>2</span> <span class=p>:</span> <span class=k>index</span><span class=p>}</span>
  <span class=p>:</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>i64</span><span class=p>&gt;</span> jointly <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>f32</span><span class=p>&gt;</span>
</code></pre></div><h4 id=attributes-4>Attributes:&nbsp;<a class=headline-hash href=#attributes-4>Â¶</a></h4><table><thead><tr><th style=text-align:center>Attribute</th><th style=text-align:center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>nx</code></td><td style=text-align:center>::mlir::IntegerAttr</td><td>index attribute</td></tr><tr><td style=text-align:center><code>ny</code></td><td style=text-align:center>::mlir::IntegerAttr</td><td>index attribute</td></tr><tr><td style=text-align:center><code>stable</code></td><td style=text-align:center>::mlir::UnitAttr</td><td>unit attribute</td></tr></tbody></table><h4 id=operands-14>Operands:&nbsp;<a class=headline-hash href=#operands-14>Â¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>n</code></td><td>index</td></tr><tr><td style=text-align:center><code>xy</code></td><td>1D memref of integer or index values</td></tr><tr><td style=text-align:center><code>ys</code></td><td>1D memref of any type values</td></tr></tbody></table><h3 id=sparse_tensorsort-mlirsparse_tensorsortop><code>sparse_tensor.sort</code> (::mlir::sparse_tensor::SortOp)&nbsp;<a class=headline-hash href=#sparse_tensorsort-mlirsparse_tensorsortop>Â¶</a></h3><p>Sorts the arrays in xs and ys lexicographically on the integral values found in the xs list</p><p>Syntax:</p><pre><code>operation ::= `sparse_tensor.sort` (`stable` $stable^)? $n`,`$xs (`jointly` $ys^)? attr-dict`:` type($xs) (`jointly` type($ys)^)?
</code></pre><p>Lexicographically sort the first <code>n</code> values in <code>xs</code> along with the values in
<code>ys</code>. Conceptually, the values being sorted are tuples produced by
<code>zip(zip(xs), zip(ys))</code>. In particular, values in <code>ys</code> needed to be sorted
along with values in <code>xs</code>, but values in <code>ys</code> don&rsquo;t affect the
lexicographical order. The order in which arrays appear in <code>xs</code> affects the
sorting result. The operator updates <code>xs</code> and <code>ys</code> in place with the result
of the sorting.</p><p>For example, assume x1=[4, 3], x2=[1, 2], y1=[10, 5], then the output of
&ldquo;sort 2, x1, x2 jointly y1&rdquo; are x1=[3, 4], x2=[2, 1], y1=[5, 10] while the
output of &ldquo;sort 2, x2, x1, jointly y1&rdquo; are x2=[1, 2], x1=[4, 3], y1=[10, 5].</p><p>Buffers in <code>xs</code> needs to have the same integral element type while buffers
in <code>ys</code> can have different numeric element types. All buffers in <code>xs</code> and
<code>ys</code> should have a dimension not less than <code>n</code>. The behavior of the operator
is undefined if this condition is not met. The operator requires at least
one buffer in <code>xs</code> while <code>ys</code> can be empty.</p><p>The <code>stable</code> attribute indicates whether a stable sorting algorithm should
be used to implement the operator.</p><p>Note that this operation is &ldquo;impure&rdquo; in the sense that its behavior is
solely defined by side-effects and not SSA values.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir>sparse_tensor<span class=p>.</span>sort <span class=nv>%n</span><span class=p>,</span> <span class=nv>%x1</span><span class=p>,</span> <span class=nv>%x2</span> jointly y1<span class=p>,</span> <span class=nv>%y2</span>
  <span class=p>:</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;,</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;</span> jointly <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;,</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>f32</span><span class=p>&gt;</span>
</code></pre></div><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir>sparse_tensor<span class=p>.</span>sort stable <span class=nv>%n</span><span class=p>,</span> <span class=nv>%x1</span><span class=p>,</span> <span class=nv>%x2</span> jointly y1<span class=p>,</span> <span class=nv>%y2</span>
  <span class=p>:</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;,</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;</span> jointly <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;,</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>f32</span><span class=p>&gt;</span>
</code></pre></div><p>Traits: AttrSizedOperandSegments</p><h4 id=attributes-5>Attributes:&nbsp;<a class=headline-hash href=#attributes-5>Â¶</a></h4><table><thead><tr><th style=text-align:center>Attribute</th><th style=text-align:center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>stable</code></td><td style=text-align:center>::mlir::UnitAttr</td><td>unit attribute</td></tr></tbody></table><h4 id=operands-15>Operands:&nbsp;<a class=headline-hash href=#operands-15>Â¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>n</code></td><td>index</td></tr><tr><td style=text-align:center><code>xs</code></td><td>1D memref of integer or index values</td></tr><tr><td style=text-align:center><code>ys</code></td><td>1D memref of any type values</td></tr></tbody></table><h3 id=sparse_tensorindices-mlirsparse_tensortoindicesop><code>sparse_tensor.indices</code> (::mlir::sparse_tensor::ToIndicesOp)&nbsp;<a class=headline-hash href=#sparse_tensorindices-mlirsparse_tensortoindicesop>Â¶</a></h3><p>Extracts indices array at given dimension from a tensor</p><p>Syntax:</p><pre><code>operation ::= `sparse_tensor.indices` $tensor attr-dict `:` type($tensor) `to` type($result)
</code></pre><p>Returns the indices array of the sparse storage format at the
given dimension for the given sparse tensor. This is similar to the
<code>bufferization.to_memref</code> operation in the sense that it provides a bridge
between a tensor world view and a bufferized world view. Unlike the
<code>bufferization.to_memref</code> operation, however, this sparse operation actually
lowers into code that extracts the indices array from the sparse storage
scheme (either by calling a support library or through direct code).</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%1</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>indices <span class=nv>%0</span> <span class=p>{</span> <span class=nl>dimension =</span> <span class=m>1</span> <span class=p>:</span> <span class=k>index</span> <span class=p>}</span>
   <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>64x64x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span> to <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;</span>
</code></pre></div><p>Traits: AlwaysSpeculatableImplTrait</p><p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)</p><p>Effects: MemoryEffects::Effect{}</p><h4 id=attributes-6>Attributes:&nbsp;<a class=headline-hash href=#attributes-6>Â¶</a></h4><table><thead><tr><th style=text-align:center>Attribute</th><th style=text-align:center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>dimension</code></td><td style=text-align:center>::mlir::IntegerAttr</td><td>index attribute</td></tr></tbody></table><h4 id=operands-16>Operands:&nbsp;<a class=headline-hash href=#operands-16>Â¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>tensor</code></td><td>sparse tensor of any type values</td></tr></tbody></table><h4 id=results-13>Results:&nbsp;<a class=headline-hash href=#results-13>Â¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>strided memref of any type values of rank 1</td></tr></tbody></table><h3 id=sparse_tensorpointers-mlirsparse_tensortopointersop><code>sparse_tensor.pointers</code> (::mlir::sparse_tensor::ToPointersOp)&nbsp;<a class=headline-hash href=#sparse_tensorpointers-mlirsparse_tensortopointersop>Â¶</a></h3><p>Extracts pointers array at given dimension from a tensor</p><p>Syntax:</p><pre><code>operation ::= `sparse_tensor.pointers` $tensor attr-dict `:` type($tensor) `to` type($result)
</code></pre><p>Returns the pointers array of the sparse storage format at the
given dimension for the given sparse tensor. This is similar to the
<code>bufferization.to_memref</code> operation in the sense that it provides a bridge
between a tensor world view and a bufferized world view. Unlike the
<code>bufferization.to_memref</code> operation, however, this sparse operation actually
lowers into code that extracts the pointers array from the sparse storage
scheme (either by calling a support library or through direct code).</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%1</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>pointers <span class=nv>%0</span> <span class=p>{</span> <span class=nl>dimension =</span> <span class=m>1</span> <span class=p>:</span> <span class=k>index</span> <span class=p>}</span>
   <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>64x64x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span> to <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;</span>
</code></pre></div><p>Traits: AlwaysSpeculatableImplTrait</p><p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)</p><p>Effects: MemoryEffects::Effect{}</p><h4 id=attributes-7>Attributes:&nbsp;<a class=headline-hash href=#attributes-7>Â¶</a></h4><table><thead><tr><th style=text-align:center>Attribute</th><th style=text-align:center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>dimension</code></td><td style=text-align:center>::mlir::IntegerAttr</td><td>index attribute</td></tr></tbody></table><h4 id=operands-17>Operands:&nbsp;<a class=headline-hash href=#operands-17>Â¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>tensor</code></td><td>sparse tensor of any type values</td></tr></tbody></table><h4 id=results-14>Results:&nbsp;<a class=headline-hash href=#results-14>Â¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>strided memref of any type values of rank 1</td></tr></tbody></table><h3 id=sparse_tensorvalues-mlirsparse_tensortovaluesop><code>sparse_tensor.values</code> (::mlir::sparse_tensor::ToValuesOp)&nbsp;<a class=headline-hash href=#sparse_tensorvalues-mlirsparse_tensortovaluesop>Â¶</a></h3><p>Extracts numerical values array from a tensor</p><p>Syntax:</p><pre><code>operation ::= `sparse_tensor.values` $tensor attr-dict `:` type($tensor) `to` type($result)
</code></pre><p>Returns the values array of the sparse storage format for the given
sparse tensor, independent of the actual dimension. This is similar to
the <code>bufferization.to_memref</code> operation in the sense that it provides a bridge
between a tensor world view and a bufferized world view. Unlike the
<code>bufferization.to_memref</code> operation, however, this sparse operation actually
lowers into code that extracts the values array from the sparse storage
scheme (either by calling a support library or through direct code).</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%1</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>values <span class=nv>%0</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>64x64x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span> to <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>f64</span><span class=p>&gt;</span>
</code></pre></div><p>Traits: AlwaysSpeculatableImplTrait</p><p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)</p><p>Effects: MemoryEffects::Effect{}</p><h4 id=operands-18>Operands:&nbsp;<a class=headline-hash href=#operands-18>Â¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>tensor</code></td><td>sparse tensor of any type values</td></tr></tbody></table><h4 id=results-15>Results:&nbsp;<a class=headline-hash href=#results-15>Â¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>strided memref of any type values of rank 1</td></tr></tbody></table><h3 id=sparse_tensorunary-mlirsparse_tensorunaryop><code>sparse_tensor.unary</code> (::mlir::sparse_tensor::UnaryOp)&nbsp;<a class=headline-hash href=#sparse_tensorunary-mlirsparse_tensorunaryop>Â¶</a></h3><p>Unary set operation utilized within linalg.generic</p><p>Syntax:</p><pre><code>operation ::= `sparse_tensor.unary` $x attr-dict `:` type($x) `to` type($output) `\n`
              `present` `=` $presentRegion `\n`
              `absent` `=` $absentRegion
</code></pre><p>Defines a computation with a <code>linalg.generic</code> operation that takes a single
operand and executes one of two regions depending on whether the operand is
nonzero (i.e. stored explicitly in the sparse storage format).</p><p>Two regions are defined for the operation must appear in this order:</p><ul><li>present (elements present in the sparse tensor)</li><li>absent (elements not present in the sparse tensor)</li></ul><p>Each region contains a single block describing the computation and result.
A non-empty block must end with a sparse_tensor.yield and the return type
must match the type of <code>output</code>. The primary region&rsquo;s block has one
argument, while the missing region&rsquo;s block has zero arguments.</p><p>A region may also be declared empty (i.e. <code>absent={}</code>), indicating that the
region does not contribute to the output.</p><p>Example of A+1, restricted to existing elements:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%C</span> <span class=p>=</span> bufferization<span class=p>.</span>alloc_tensor<span class=p>...</span>
<span class=nv>%0</span> <span class=p>=</span> linalg<span class=p>.</span>generic <span class=nv>#trait</span>
   ins<span class=p>(</span><span class=nv>%A</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#SparseVector</span><span class=p>&gt;)</span>
  outs<span class=p>(</span><span class=nv>%C</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#SparseVector</span><span class=p>&gt;)</span> <span class=p>{</span>
  <span class=nl>^bb0</span><span class=p>(</span><span class=nv>%a</span><span class=p>:</span> <span class=k>f64</span><span class=p>,</span> <span class=nv>%c</span><span class=p>:</span> <span class=k>f64</span><span class=p>)</span> <span class=p>:</span>
    <span class=nv>%result</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>unary <span class=nv>%a</span> <span class=p>:</span> <span class=k>f64</span> to <span class=k>f64</span>
      <span class=nl>present=</span><span class=p>{</span>
        <span class=nl>^bb0</span><span class=p>(</span><span class=nv>%arg0</span><span class=p>:</span> <span class=k>f64</span><span class=p>):</span>
          <span class=nv>%cf1</span> <span class=p>=</span> arith<span class=p>.</span><span class=kt>constant</span> <span class=m>1.0</span> <span class=p>:</span> <span class=k>f64</span>
          <span class=nv>%ret</span> <span class=p>=</span> arith<span class=p>.</span>addf <span class=nv>%arg0</span><span class=p>,</span> <span class=nv>%cf1</span> <span class=p>:</span> <span class=k>f64</span>
          sparse_tensor<span class=p>.</span>yield <span class=nv>%ret</span> <span class=p>:</span> <span class=k>f64</span>
      <span class=p>}</span>
      <span class=nl>absent=</span><span class=p>{}</span>
    linalg<span class=p>.</span>yield <span class=nv>%result</span> <span class=p>:</span> <span class=k>f64</span>
<span class=p>}</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#SparseVector</span><span class=p>&gt;</span>
</code></pre></div><p>Example returning +1 for existing values and -1 for missing values:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%result</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>unary <span class=nv>%a</span> <span class=p>:</span> <span class=k>f64</span> to <span class=k>i32</span>
  <span class=nl>present=</span><span class=p>{</span>
    <span class=nl>^bb0</span><span class=p>(</span><span class=nv>%x</span><span class=p>:</span> <span class=k>f64</span><span class=p>):</span>
      <span class=nv>%ret</span> <span class=p>=</span> arith<span class=p>.</span><span class=kt>constant</span> <span class=m>1</span> <span class=p>:</span> <span class=k>i32</span>
      sparse_tensor<span class=p>.</span>yield <span class=nv>%ret</span> <span class=p>:</span> <span class=k>i32</span>
  <span class=p>}</span>
  <span class=nl>absent=</span><span class=p>{</span>
    <span class=nv>%ret</span> <span class=p>=</span> arith<span class=p>.</span><span class=kt>constant</span> <span class=m>-1</span> <span class=p>:</span> <span class=k>i32</span>
    sparse_tensor<span class=p>.</span>yield <span class=nv>%ret</span> <span class=p>:</span> <span class=k>i32</span>
  <span class=p>}</span>
</code></pre></div><p>Example showing a structural inversion (existing values become missing in
the output, while missing values are filled with 1):</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%result</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>unary <span class=nv>%a</span> <span class=p>:</span> <span class=k>f64</span> to <span class=k>i64</span>
  <span class=nl>present=</span><span class=p>{}</span>
  <span class=nl>absent=</span><span class=p>{</span>
    <span class=nv>%ret</span> <span class=p>=</span> arith<span class=p>.</span><span class=kt>constant</span> <span class=m>1</span> <span class=p>:</span> <span class=k>i64</span>
    sparse_tensor<span class=p>.</span>yield <span class=nv>%ret</span> <span class=p>:</span> <span class=k>i64</span>
  <span class=p>}</span>
</code></pre></div><p>Traits: AlwaysSpeculatableImplTrait</p><p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)</p><p>Effects: MemoryEffects::Effect{}</p><h4 id=operands-19>Operands:&nbsp;<a class=headline-hash href=#operands-19>Â¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>x</code></td><td>any type</td></tr></tbody></table><h4 id=results-16>Results:&nbsp;<a class=headline-hash href=#results-16>Â¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>output</code></td><td>any type</td></tr></tbody></table><h3 id=sparse_tensoryield-mlirsparse_tensoryieldop><code>sparse_tensor.yield</code> (::mlir::sparse_tensor::YieldOp)&nbsp;<a class=headline-hash href=#sparse_tensoryield-mlirsparse_tensoryieldop>Â¶</a></h3><p>Yield from sparse_tensor set-like operations</p><p>Syntax:</p><pre><code>operation ::= `sparse_tensor.yield` $result attr-dict `:` type($result)
</code></pre><p>Yields a value from within a <code>binary</code>, <code>unary</code>, <code>reduce</code>,
<code>select</code> or <code>foreach</code> block.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%0</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>unary <span class=nv>%a</span> <span class=p>:</span> <span class=k>i64</span> to <span class=k>i64</span> <span class=p>{</span>
  <span class=nl>^bb0</span><span class=p>(</span><span class=nv>%arg0</span><span class=p>:</span> <span class=k>i64</span><span class=p>):</span>
    <span class=nv>%cst</span> <span class=p>=</span> arith<span class=p>.</span><span class=kt>constant</span> <span class=m>1</span> <span class=p>:</span> <span class=k>i64</span>
    <span class=nv>%ret</span> <span class=p>=</span> arith<span class=p>.</span>addi <span class=nv>%arg0</span><span class=p>,</span> <span class=nv>%cst</span> <span class=p>:</span> <span class=k>i64</span>
    sparse_tensor<span class=p>.</span>yield <span class=nv>%ret</span> <span class=p>:</span> <span class=k>i64</span>
<span class=p>}</span>
</code></pre></div><p>Traits: AlwaysSpeculatableImplTrait, Terminator</p><p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)</p><p>Effects: MemoryEffects::Effect{}</p><h4 id=operands-20>Operands:&nbsp;<a class=headline-hash href=#operands-20>Â¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>any type</td></tr></tbody></table><h2 id=attribute-definition>Attribute definition&nbsp;<a class=headline-hash href=#attribute-definition>Â¶</a></h2><h3 id=sparsetensorencodingattr>SparseTensorEncodingAttr&nbsp;<a class=headline-hash href=#sparsetensorencodingattr>Â¶</a></h3><p>An attribute to encode TACO-style information on sparsity properties
of tensors. The encoding is eventually used by a <strong>sparse compiler</strong>
pass to generate sparse code fully automatically for all tensor
expressions that involve tensors with a sparse encoding. Compiler
passes that run before this sparse compiler pass need to be
aware of the semantics of tensor types with such an encoding.</p><p>The attribute consists of the following fields.</p><ul><li><p>Dimension level type for each dimension of a tensor type:</p><ul><li><strong>dense</strong> : dimension is dense, all entries along this dimension
are stored</li><li><strong>compressed</strong> : dimension is sparse, only nonzeros along this dimensions
are stored</li><li><strong>singleton</strong> : dimension stores individual indices with no siblings
By default, each dimension level types has the property of being unique
(no duplicates at that level) and ordered (indices appear sorted at that
level). The following two suffixes can be used to make the last two
dimension level types not-unique (duplicates may appear) and not-ordered
(indices may appear unsorted).</li><li><strong>-nu</strong> : not unique</li><li><strong>-no</strong> : not ordered
Currently, these suffixes, is present, should appear in this order.
In the future, we may introduce many more dimension level types and
properties, and separate specifying the two completely rather than
using this suffix mechanism.</li></ul></li><li><p>An optional dimension ordering on the indices of this tensor type. Unlike
dense storage, most sparse storage schemes do not provide fast random
access. This affine map specifies the order of dimensions that should be
supported by the sparse storage scheme. For example, for a 2-d tensor,
<code>(i, j) -> (i, j)</code> requests row-wise storage and <code>(i, j) -> (j, i)</code>
requests column-wise storage. By default, an identify mapping is used,
which implies that the original indices directly correspond to stored
indices.</p></li><li><p>An optional higher-ordering mapping from the original index space of
the tensor to a higher-order index space, used to define block-sparse
storage or ELL (jagged diagonal) storage. For example, for a 2-d tensor,
the mapping <code>(i, j) -> (i floordiv 2, j floordiv 3, i mod 2, j mod 3)</code>
imposes an higher-order partitioning into 2x3 blocks along the matrix
layout. A dimension ordering can be used to define a desired ordering
on this higher-order index space. Likewise, the dimension level types
define dense or compressed storage along this higher-order index space.
For block-sparse, blocks are typically stored with compression while
dense storage is used within each block (although hybrid schemes are
possible as well). The higher-order mapping also provides a notion of
&ldquo;counting a dimension&rdquo;, where every stored element with the same index
is mapped to a new slice. For instance, ELL storage of a 2-d tensor can
be defined with the mapping <code>(i, j) -> (#i, i, j)</code> using the notation
of [Chou20]. Lacking the <code>#</code> symbol in MLIR&rsquo;s affine mapping, we use
a free symbol <code>c</code> to define such counting, together with a constant
that denotes the number of resulting slices. For example, the mapping
<code>(i, j)[c] -> (c * 3 * i, i, j)</code> with the first two higher-order indices
stored dense and the innermost compressed denotes ELL storage with
three jagged diagonals that count the dimension <code>i</code>.</p><p>TODO: introduce a real counting symbol to MLIR&rsquo;s mapping, since an
expression like 3<em>c</em>i has no direct interpretation?</p></li><li><p>The required bit width for &ldquo;pointer&rdquo; storage (integral offsets into
the sparse storage scheme). A narrow width reduces the memory footprint
of overhead storage, as long as the width suffices to define the total
required range (viz. the maximum number of stored entries over all indirection
dimensions). The choices are <code>8</code>, <code>16</code>, <code>32</code>, <code>64</code>, or, the default, <code>0</code> to
indicate the native bit width.</p></li><li><p>The required bit width for &ldquo;index&rdquo; storage (elements of the coordinates of
stored entries). A narrow width reduces the memory footprint of overhead
storage, as long as the width suffices to define the total required range
(viz. the maximum value of each tensor index over all dimensions). The
choices are <code>8</code>, <code>16</code>, <code>32</code>, <code>64</code>, or, the default, <code>0</code> to indicate a
native bit width.</p></li></ul><p>Examples:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=c>// Sparse vector.
</span><span class=c></span><span class=nv>#SparseVector</span> <span class=p>=</span> <span class=nv>#sparse_tensor.encoding</span><span class=p>&lt;{</span>
  <span class=nl>dimLevelType =</span> <span class=p>[</span> <span class=s>&#34;compressed&#34;</span> <span class=p>]</span>
<span class=p>}&gt;</span>
<span class=p>...</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>f32</span><span class=p>,</span> <span class=nv>#SparseVector</span><span class=p>&gt;</span> <span class=p>...</span>

<span class=c>// Sorted Coordinate Scheme.
</span><span class=c></span><span class=nv>#SortedCOO</span> <span class=p>=</span> <span class=nv>#sparse_tensor.encoding</span><span class=p>&lt;{</span>
  <span class=nl>dimLevelType =</span> <span class=p>[</span> <span class=s>&#34;compressed-nu&#34;</span><span class=p>,</span> <span class=s>&#34;singleton&#34;</span> <span class=p>]</span>
<span class=p>}&gt;</span>
<span class=p>...</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#SortedCOO</span><span class=p>&gt;</span> <span class=p>...</span>

<span class=c>// Doubly compressed sparse column storage with specific bitwidths.
</span><span class=c></span><span class=nv>#DCSC</span> <span class=p>=</span> <span class=nv>#sparse_tensor.encoding</span><span class=p>&lt;{</span>
  <span class=nl>dimLevelType =</span> <span class=p>[</span> <span class=s>&#34;compressed&#34;</span><span class=p>,</span> <span class=s>&#34;compressed&#34;</span> <span class=p>],</span>
  <span class=nl>dimOrdering =</span> affine_map<span class=p>&lt;(</span>i<span class=p>,</span> j<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>j<span class=p>,</span> i<span class=p>)&gt;,</span>
  <span class=nl>pointerBitWidth =</span> <span class=m>32</span><span class=p>,</span>
  <span class=nl>indexBitWidth =</span> <span class=m>8</span>
<span class=p>}&gt;</span>
<span class=p>...</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>8x8x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#DCSC</span><span class=p>&gt;</span> <span class=p>...</span>

<span class=c>// Block sparse row storage (2x3 blocks).
</span><span class=c></span><span class=nv>#BCSR</span> <span class=p>=</span> <span class=nv>#sparse_tensor.encoding</span><span class=p>&lt;{</span>
  <span class=nl>dimLevelType =</span> <span class=p>[</span> <span class=s>&#34;compressed&#34;</span><span class=p>,</span> <span class=s>&#34;compressed&#34;</span><span class=p>,</span> <span class=s>&#34;dense&#34;</span><span class=p>,</span> <span class=s>&#34;dense&#34;</span> <span class=p>],</span>
  <span class=nl>dimOrdering  =</span> affine_map<span class=p>&lt;(</span>ii<span class=p>,</span> jj<span class=p>,</span> i<span class=p>,</span> j<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>ii<span class=p>,</span> jj<span class=p>,</span> i<span class=p>,</span> j<span class=p>)&gt;,</span>
  <span class=nl>higherOrdering =</span> affine_map<span class=p>&lt;(</span>i<span class=p>,</span> j<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>i floordiv <span class=m>2</span><span class=p>,</span> j floordiv <span class=m>3</span><span class=p>,</span> i mod <span class=m>2</span><span class=p>,</span> j mod <span class=m>3</span><span class=p>)&gt;</span>
<span class=p>}&gt;</span>
<span class=p>...</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>20x30x</span><span class=k>f32</span><span class=p>,</span> <span class=nv>#BCSR</span><span class=p>&gt;</span> <span class=p>...</span>

<span class=c>// ELL storage (4 jagged diagonals, i.e., at most 4 nonzeros per row).
</span><span class=c></span><span class=nv>#ELL</span> <span class=p>=</span> <span class=nv>#sparse_tensor.encoding</span><span class=p>&lt;{</span>
  <span class=nl>dimLevelType =</span> <span class=p>[</span> <span class=s>&#34;dense&#34;</span><span class=p>,</span> <span class=s>&#34;dense&#34;</span><span class=p>,</span> <span class=s>&#34;compressed&#34;</span> <span class=p>],</span>
  <span class=nl>dimOrdering  =</span> affine_map<span class=p>&lt;(</span>ii<span class=p>,</span> i<span class=p>,</span> j<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>ii<span class=p>,</span> i<span class=p>,</span> j<span class=p>)&gt;,</span>
  <span class=nl>higherOrdering =</span> affine_map<span class=p>&lt;(</span>i<span class=p>,</span> j<span class=p>)[</span>c<span class=p>]</span> <span class=p>-&gt;</span> <span class=p>(</span>c <span class=p>*</span> <span class=m>4</span> <span class=p>*</span> i<span class=p>,</span> i<span class=p>,</span> j<span class=p>)&gt;</span>
<span class=p>}&gt;</span>
<span class=p>...</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#ELL</span><span class=p>&gt;</span> <span class=p>...</span>
</code></pre></div><h4 id=parameters>Parameters:&nbsp;<a class=headline-hash href=#parameters>Â¶</a></h4><table><thead><tr><th style=text-align:center>Parameter</th><th style=text-align:center>C++ type</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center>dimLevelType</td><td style=text-align:center><code>::llvm::ArrayRef&lt;::mlir::sparse_tensor::DimLevelType></code></td><td>per dimension level type</td></tr><tr><td style=text-align:center>dimOrdering</td><td style=text-align:center><code>AffineMap</code></td><td></td></tr><tr><td style=text-align:center>higherOrdering</td><td style=text-align:center><code>AffineMap</code></td><td></td></tr><tr><td style=text-align:center>pointerBitWidth</td><td style=text-align:center><code>unsigned</code></td><td></td></tr><tr><td style=text-align:center>indexBitWidth</td><td style=text-align:center><code>unsigned</code></td><td></td></tr></tbody></table><div class=edit-meta><br></div><nav class=pagination><a class="nav nav-prev" href=/docs/Dialects/ShapeDialect/ title="'shape' Dialect"><i class="fas fa-arrow-left" aria-hidden=true></i>Prev - 'shape' Dialect</a>
<a class="nav nav-next" href=/docs/Dialects/TensorOps/ title="'tensor' Dialect">Next - 'tensor' Dialect <i class="fas fa-arrow-right" aria-hidden=true></i></a></nav><footer><p class=powered>Powered by <a href=https://gohugo.io>Hugo</a>. Theme by <a href=https://themes.gohugo.io/hugo-theme-techdoc/>TechDoc</a>. Designed by <a href=https://github.com/thingsym/hugo-theme-techdoc>Thingsym</a>.</p></footer></main><div class=sidebar><nav class=slide-menu><ul><li><a href=https://mlir.llvm.org/>Home</a></li><li><a href=/pubs/>MLIR Related Publications</a></li><li><a href=/talks/>Talks</a></li><li><a href=/users/>Users of MLIR</a></li><li class=has-sub-menu><a href=/getting_started/>Getting Started<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/getting_started/ReportingIssues/>Reporting Issues</a></li><li><a href=/getting_started/Debugging/>Debugging Tips</a></li><li><a href=/getting_started/Faq/>FAQ</a></li><li><a href=/getting_started/Contributing/>How to Contribute</a></li><li><a href=/getting_started/DeveloperGuide/>Developer Guide</a></li><li><a href=/getting_started/openprojects/>Open Projects</a></li><li><a href=/getting_started/Glossary/>Glossary</a></li><li><a href=/getting_started/TestingGuide/>Testing Guide</a></li></ul></li><li class="parent has-sub-menu"><a href=/docs/>Code Documentation<span class="mark opened">-</span></a><ul class=sub-menu><li class=has-sub-menu><a href=/docs/Bindings/>Bindings<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Bindings/Python/>MLIR Python Bindings</a></li></ul></li><li class=has-sub-menu><a href=/docs/Tools/>Tools<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Tools/MLIRLSP/>MLIR : Language Server Protocol</a></li><li><a href=/docs/Tools/mlir-reduce/>MLIR Reduce</a></li></ul></li><li><a href=/docs/NVGPUPasses/></a></li><li><a href=/docs/BufferDeallocationInternals/>Buffer Deallocation - Internals</a></li><li><a href=/docs/Bufferization/>Bufferization</a></li><li><a href=/docs/DataLayout/>Data Layout Modeling</a></li><li><a href=/docs/DebugActions/>Debug Actions</a></li><li><a href=/docs/AttributesAndTypes/>Defining Dialect Attributes and Types</a></li><li><a href=/docs/DefiningDialects/>Defining Dialects</a></li><li><a href=/docs/Diagnostics/>Diagnostic Infrastructure</a></li><li><a href=/docs/DialectConversion/>Dialect Conversion</a></li><li class="parent has-sub-menu"><a href=/docs/Dialects/>Dialects<span class="mark opened">-</span></a><ul class=sub-menu><li><a href=/docs/Dialects/MemRefTransformOps/></a></li><li><a href=/docs/Dialects/OpenACCDialect/>'acc' Dialect</a></li><li><a href=/docs/Dialects/Affine/>'affine' Dialect</a></li><li><a href=/docs/Dialects/AMDGPU/>'amdgpu' Dialect</a></li><li><a href=/docs/Dialects/AMX/>'amx' Dialect</a></li><li><a href=/docs/Dialects/ArithOps/>'arith' Dialect</a></li><li><a href=/docs/Dialects/ArmNeon/>'arm_neon' Dialect</a></li><li><a href=/docs/Dialects/ArmSVE/>'arm_sve' Dialect</a></li><li><a href=/docs/Dialects/AsyncDialect/>'async' Dialect</a></li><li><a href=/docs/Dialects/BufferizationOps/>'bufferization' Dialect</a></li><li><a href=/docs/Dialects/ControlFlowDialect/>'cf' Dialect</a></li><li><a href=/docs/Dialects/ComplexOps/>'complex' Dialect</a></li><li><a href=/docs/Dialects/DLTIDialect/>'dlti' Dialect</a></li><li><a href=/docs/Dialects/EmitC/>'emitc' Dialect</a></li><li><a href=/docs/Dialects/Func/>'func' Dialect</a></li><li><a href=/docs/Dialects/GPU/>'gpu' Dialect</a></li><li><a href=/docs/Dialects/IndexOps/>'index' Dialect</a></li><li class=has-sub-menu><a href=/docs/Dialects/Linalg/>'linalg' Dialect<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Dialects/Linalg/OpDSL/>Linalg OpDSL</a></li></ul></li><li><a href=/docs/Dialects/LLVM/>'llvm' Dialect</a></li><li><a href=/docs/Dialects/MathOps/>'math' Dialect</a></li><li><a href=/docs/Dialects/MemRef/>'memref' Dialect</a></li><li><a href=/docs/Dialects/MLProgramOps/>'ml_program' Dialect</a></li><li><a href=/docs/Dialects/NVGPU/>'nvgpu' Dialect</a></li><li><a href=/docs/Dialects/NVVMDialect/>'nvvm' Dialect</a></li><li><a href=/docs/Dialects/OpenMPDialect/>'omp' Dialect</a></li><li><a href=/docs/Dialects/PDLOps/>'pdl' Dialect</a></li><li><a href=/docs/Dialects/PDLInterpOps/>'pdl_interp' Dialect</a></li><li><a href=/docs/Dialects/QuantDialect/>'quant' Dialect</a></li><li><a href=/docs/Dialects/ROCDLDialect/>'rocdl' Dialect</a></li><li><a href=/docs/Dialects/SCFDialect/>'scf' Dialect</a></li><li><a href=/docs/Dialects/ShapeDialect/>'shape' Dialect</a></li><li class=active><a href=/docs/Dialects/SparseTensorOps/>'sparse_tensor' Dialect</a></li><li><a href=/docs/Dialects/TensorOps/>'tensor' Dialect</a></li><li><a href=/docs/Dialects/Vector/>'vector' Dialect</a></li><li><a href=/docs/Dialects/X86Vector/>'x86vector' Dialect</a></li><li><a href=/docs/Dialects/Builtin/>Builtin Dialect</a></li><li><a href=/docs/Dialects/SPIR-V/>SPIR-V Dialect</a></li><li><a href=/docs/Dialects/TOSA/>Tensor Operator Set Architecture (TOSA) Dialect</a></li><li><a href=/docs/Dialects/Transform/>Transform Dialect</a></li></ul></li><li><a href=/docs/Interfaces/>Interfaces</a></li><li><a href=/docs/TargetLLVMIR/>LLVM IR Target</a></li><li><a href=/docs/BytecodeFormat/>MLIR Bytecode Format</a></li><li><a href=/docs/CAPI/>MLIR C API</a></li><li><a href=/docs/LangRef/>MLIR Language Reference</a></li><li><a href=/docs/Canonicalization/>Operation Canonicalization</a></li><li><a href=/docs/OpDefinitions/>Operation Definition Specification (ODS)</a></li><li><a href=/docs/PassManagement/>Pass Infrastructure</a></li><li><a href=/docs/Passes/>Passes</a></li><li><a href=/docs/PatternRewriter/>Pattern Rewriting : Generic DAG-to-DAG Rewriting</a></li><li><a href=/docs/PDLL/>PDLL - PDL Language</a></li><li><a href=/docs/Quantization/>Quantization</a></li><li class=has-sub-menu><a href=/docs/Rationale/>Rationale<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Rationale/RationaleGenericDAGRewriter/>Generic DAG Rewriter Infrastructure Rationale</a></li><li><a href=/docs/Rationale/RationaleLinalgDialect/>Linalg Dialect Rationale: The Case For Compiler-Friendly Custom Operations</a></li><li><a href=/docs/Rationale/Rationale/>MLIR Rationale</a></li><li><a href=/docs/Rationale/MLIRForGraphAlgorithms/>MLIR: Incremental Application to Graph Algorithms in ML Frameworks</a></li><li><a href=/docs/Rationale/RationaleSimplifiedPolyhedralForm/>MLIR: The case for a simplified polyhedral form</a></li><li><a href=/docs/Rationale/SideEffectsAndSpeculation/>Side Effects & Speculation</a></li><li><a href=/docs/Rationale/UsageOfConst/>Usage of 'const' in MLIR, for core IR types</a></li></ul></li><li><a href=/docs/ShapeInference/>Shape Inference</a></li><li><a href=/docs/SPIRVToLLVMDialectConversion/>SPIR-V Dialect to LLVM Dialect conversion manual</a></li><li><a href=/docs/SymbolsAndSymbolTables/>Symbols and Symbol Tables</a></li><li><a href=/docs/DeclarativeRewrites/>Table-driven Declarative Rewrite Rule (DRR)</a></li><li><a href=/docs/Traits/>Traits</a></li><li class=has-sub-menu><a href=/docs/Tutorials/>Tutorials<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Tutorials/CreatingADialect/>Creating a Dialect</a></li><li><a href=/docs/Tutorials/QuickstartRewrites/>Quickstart tutorial to adding MLIR graph rewrite</a></li><li class=has-sub-menu><a href=/docs/Tutorials/Toy/>Toy Tutorial<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Tutorials/Toy/Ch-1/>Chapter 1: Toy Language and AST</a></li><li><a href=/docs/Tutorials/Toy/Ch-2/>Chapter 2: Emitting Basic MLIR</a></li><li><a href=/docs/Tutorials/Toy/Ch-3/>Chapter 3: High-level Language-Specific Analysis and Transformation</a></li><li><a href=/docs/Tutorials/Toy/Ch-4/>Chapter 4: Enabling Generic Transformation with Interfaces</a></li><li><a href=/docs/Tutorials/Toy/Ch-5/>Chapter 5: Partial Lowering to Lower-Level Dialects for Optimization</a></li><li><a href=/docs/Tutorials/Toy/Ch-6/>Chapter 6: Lowering to LLVM and CodeGeneration</a></li><li><a href=/docs/Tutorials/Toy/Ch-7/>Chapter 7: Adding a Composite Type to Toy</a></li></ul></li><li><a href=/docs/Tutorials/UnderstandingTheIRStructure/>Understanding the IR Structure</a></li><li><a href=/docs/Tutorials/DataFlowAnalysis/>Writing DataFlow Analyses in MLIR</a></li></ul></li></ul></li></ul></nav><div class=sidebar-footer></div></div></div><a href=# id=backtothetop-fixed class=backtothetop data-backtothetop-duration=600 data-backtothetop-easing=easeOutQuart data-backtothetop-fixed-fadein=1000 data-backtothetop-fixed-fadeout=1000 data-backtothetop-fixed-bottom=10 data-backtothetop-fixed-right=20><span class="fa-layers fa-fw"><i class="fas fa-circle"></i><i class="fas fa-arrow-circle-up"></i></span></a></div></body></html>